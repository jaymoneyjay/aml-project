{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3979addb",
   "metadata": {},
   "source": [
    "# Build model for different features\n",
    "- HB Features\n",
    "- Delineation Features\n",
    "- HRV Features\n",
    "- CNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a6e67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set('talk')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398e557",
   "metadata": {},
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3db0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6cd75d",
   "metadata": {},
   "source": [
    "Read in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da2b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/rapwag01/eth/aml/aml-project/task2/data/'\n",
    "\n",
    "X_train_hb = pd.read_csv(path+'first_run/features_heartbeat_train.csv', index_col=False).values\n",
    "X_test_hb = pd.read_csv(path+'first_run/features_heartbeat_test.csv', index_col=False).values\n",
    "\n",
    "X_train_cwt = pd.read_csv(path+'features_cwt_train.csv', index_col=False).values\n",
    "X_test_cwt = pd.read_csv(path+'features_cwt_test.csv', index_col=False).values\n",
    "\n",
    "X_train_delin = pd.read_csv(path+'first_run/features_delineation_train.csv', index_col=False).values\n",
    "X_test_delin = pd.read_csv(path+'first_run/features_delineation_test.csv', index_col=False).values\n",
    "\n",
    "X_train_hrv = pd.read_csv(path+'first_run/features_hrv_train.csv', index_col=False).values\n",
    "X_test_hrv = pd.read_csv(path+'first_run/features_hrv_test.csv', index_col=False).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ffefc7",
   "metadata": {},
   "source": [
    "Read in target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7cef2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('../../task2/y_train.csv').y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d63471f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5117,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c4ac7",
   "metadata": {},
   "source": [
    "Join feature matrices\n",
    "- Skip cwt for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e787b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = np.hstack([X_train_hb, X_train_delin, X_train_hrv])[:,1:] # drop first column which is index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce78b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_full = np.hstack([X_test_hb, X_test_delin, X_test_hrv])[:,1:] # drop first column which is index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7127823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to apply train / test split if we use gridsearchcv\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, target, random_state=0, train_size=train_size, stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483e0eb",
   "metadata": {},
   "source": [
    "remove samples with >80% NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "20b0df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = pd.DataFrame(X_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a661a0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>...</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "      <th>769</th>\n",
       "      <th>770</th>\n",
       "      <th>771</th>\n",
       "      <th>772</th>\n",
       "      <th>773</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "      <th>785</th>\n",
       "      <th>786</th>\n",
       "      <th>787</th>\n",
       "      <th>788</th>\n",
       "      <th>789</th>\n",
       "      <th>790</th>\n",
       "      <th>791</th>\n",
       "      <th>792</th>\n",
       "      <th>793</th>\n",
       "      <th>794</th>\n",
       "      <th>795</th>\n",
       "      <th>796</th>\n",
       "      <th>797</th>\n",
       "      <th>798</th>\n",
       "      <th>799</th>\n",
       "      <th>800</th>\n",
       "      <th>801</th>\n",
       "      <th>802</th>\n",
       "      <th>803</th>\n",
       "      <th>804</th>\n",
       "      <th>805</th>\n",
       "      <th>806</th>\n",
       "      <th>807</th>\n",
       "      <th>808</th>\n",
       "      <th>809</th>\n",
       "      <th>810</th>\n",
       "      <th>811</th>\n",
       "      <th>812</th>\n",
       "      <th>813</th>\n",
       "      <th>814</th>\n",
       "      <th>815</th>\n",
       "      <th>816</th>\n",
       "      <th>817</th>\n",
       "      <th>818</th>\n",
       "      <th>819</th>\n",
       "      <th>820</th>\n",
       "      <th>821</th>\n",
       "      <th>822</th>\n",
       "      <th>823</th>\n",
       "      <th>824</th>\n",
       "      <th>825</th>\n",
       "      <th>826</th>\n",
       "      <th>827</th>\n",
       "      <th>828</th>\n",
       "      <th>829</th>\n",
       "      <th>830</th>\n",
       "      <th>831</th>\n",
       "      <th>832</th>\n",
       "      <th>833</th>\n",
       "      <th>834</th>\n",
       "      <th>835</th>\n",
       "      <th>836</th>\n",
       "      <th>837</th>\n",
       "      <th>838</th>\n",
       "      <th>839</th>\n",
       "      <th>840</th>\n",
       "      <th>841</th>\n",
       "      <th>842</th>\n",
       "      <th>843</th>\n",
       "      <th>844</th>\n",
       "      <th>845</th>\n",
       "      <th>846</th>\n",
       "      <th>847</th>\n",
       "      <th>848</th>\n",
       "      <th>849</th>\n",
       "      <th>850</th>\n",
       "      <th>851</th>\n",
       "      <th>852</th>\n",
       "      <th>853</th>\n",
       "      <th>854</th>\n",
       "      <th>855</th>\n",
       "      <th>856</th>\n",
       "      <th>857</th>\n",
       "      <th>858</th>\n",
       "      <th>859</th>\n",
       "      <th>860</th>\n",
       "      <th>861</th>\n",
       "      <th>862</th>\n",
       "      <th>863</th>\n",
       "      <th>864</th>\n",
       "      <th>865</th>\n",
       "      <th>866</th>\n",
       "      <th>867</th>\n",
       "      <th>868</th>\n",
       "      <th>869</th>\n",
       "      <th>870</th>\n",
       "      <th>871</th>\n",
       "      <th>872</th>\n",
       "      <th>873</th>\n",
       "      <th>874</th>\n",
       "      <th>875</th>\n",
       "      <th>876</th>\n",
       "      <th>877</th>\n",
       "      <th>878</th>\n",
       "      <th>879</th>\n",
       "      <th>880</th>\n",
       "      <th>881</th>\n",
       "      <th>882</th>\n",
       "      <th>883</th>\n",
       "      <th>884</th>\n",
       "      <th>885</th>\n",
       "      <th>886</th>\n",
       "      <th>887</th>\n",
       "      <th>888</th>\n",
       "      <th>889</th>\n",
       "      <th>890</th>\n",
       "      <th>891</th>\n",
       "      <th>892</th>\n",
       "      <th>893</th>\n",
       "      <th>894</th>\n",
       "      <th>895</th>\n",
       "      <th>896</th>\n",
       "      <th>897</th>\n",
       "      <th>898</th>\n",
       "      <th>899</th>\n",
       "      <th>900</th>\n",
       "      <th>901</th>\n",
       "      <th>902</th>\n",
       "      <th>903</th>\n",
       "      <th>904</th>\n",
       "      <th>905</th>\n",
       "      <th>906</th>\n",
       "      <th>907</th>\n",
       "      <th>908</th>\n",
       "      <th>909</th>\n",
       "      <th>910</th>\n",
       "      <th>911</th>\n",
       "      <th>912</th>\n",
       "      <th>913</th>\n",
       "      <th>914</th>\n",
       "      <th>915</th>\n",
       "      <th>916</th>\n",
       "      <th>917</th>\n",
       "      <th>918</th>\n",
       "      <th>919</th>\n",
       "      <th>920</th>\n",
       "      <th>921</th>\n",
       "      <th>922</th>\n",
       "      <th>923</th>\n",
       "      <th>924</th>\n",
       "      <th>925</th>\n",
       "      <th>926</th>\n",
       "      <th>927</th>\n",
       "      <th>928</th>\n",
       "      <th>929</th>\n",
       "      <th>930</th>\n",
       "      <th>931</th>\n",
       "      <th>932</th>\n",
       "      <th>933</th>\n",
       "      <th>934</th>\n",
       "      <th>935</th>\n",
       "      <th>936</th>\n",
       "      <th>937</th>\n",
       "      <th>938</th>\n",
       "      <th>939</th>\n",
       "      <th>940</th>\n",
       "      <th>941</th>\n",
       "      <th>942</th>\n",
       "      <th>943</th>\n",
       "      <th>944</th>\n",
       "      <th>945</th>\n",
       "      <th>946</th>\n",
       "      <th>947</th>\n",
       "      <th>948</th>\n",
       "      <th>949</th>\n",
       "      <th>950</th>\n",
       "      <th>951</th>\n",
       "      <th>952</th>\n",
       "      <th>953</th>\n",
       "      <th>954</th>\n",
       "      <th>955</th>\n",
       "      <th>956</th>\n",
       "      <th>957</th>\n",
       "      <th>958</th>\n",
       "      <th>959</th>\n",
       "      <th>960</th>\n",
       "      <th>961</th>\n",
       "      <th>962</th>\n",
       "      <th>963</th>\n",
       "      <th>964</th>\n",
       "      <th>965</th>\n",
       "      <th>966</th>\n",
       "      <th>967</th>\n",
       "      <th>968</th>\n",
       "      <th>969</th>\n",
       "      <th>970</th>\n",
       "      <th>971</th>\n",
       "      <th>972</th>\n",
       "      <th>973</th>\n",
       "      <th>974</th>\n",
       "      <th>975</th>\n",
       "      <th>976</th>\n",
       "      <th>977</th>\n",
       "      <th>978</th>\n",
       "      <th>979</th>\n",
       "      <th>980</th>\n",
       "      <th>981</th>\n",
       "      <th>982</th>\n",
       "      <th>983</th>\n",
       "      <th>984</th>\n",
       "      <th>985</th>\n",
       "      <th>986</th>\n",
       "      <th>987</th>\n",
       "      <th>988</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-14.498234</td>\n",
       "      <td>-13.234579</td>\n",
       "      <td>-11.628894</td>\n",
       "      <td>-9.684386</td>\n",
       "      <td>-7.431202</td>\n",
       "      <td>-4.918017</td>\n",
       "      <td>-2.186781</td>\n",
       "      <td>0.719700</td>\n",
       "      <td>3.730826</td>\n",
       "      <td>6.763773</td>\n",
       "      <td>9.754217</td>\n",
       "      <td>12.654224</td>\n",
       "      <td>15.387219</td>\n",
       "      <td>17.858503</td>\n",
       "      <td>19.989768</td>\n",
       "      <td>21.740134</td>\n",
       "      <td>23.071220</td>\n",
       "      <td>23.946301</td>\n",
       "      <td>24.353872</td>\n",
       "      <td>24.313122</td>\n",
       "      <td>23.826370</td>\n",
       "      <td>22.872333</td>\n",
       "      <td>21.427582</td>\n",
       "      <td>19.466121</td>\n",
       "      <td>16.945076</td>\n",
       "      <td>13.789532</td>\n",
       "      <td>9.954404</td>\n",
       "      <td>5.429056</td>\n",
       "      <td>0.238137</td>\n",
       "      <td>-5.534010</td>\n",
       "      <td>-11.735787</td>\n",
       "      <td>-18.131505</td>\n",
       "      <td>-24.451479</td>\n",
       "      <td>-30.464007</td>\n",
       "      <td>-35.978735</td>\n",
       "      <td>-40.880341</td>\n",
       "      <td>-45.115909</td>\n",
       "      <td>-48.698300</td>\n",
       "      <td>-51.698581</td>\n",
       "      <td>-54.150485</td>\n",
       "      <td>-56.086609</td>\n",
       "      <td>-57.528309</td>\n",
       "      <td>-58.489907</td>\n",
       "      <td>-59.003520</td>\n",
       "      <td>-59.099691</td>\n",
       "      <td>-58.844840</td>\n",
       "      <td>-58.264232</td>\n",
       "      <td>-57.272099</td>\n",
       "      <td>-55.560516</td>\n",
       "      <td>-52.325388</td>\n",
       "      <td>-46.043368</td>\n",
       "      <td>-34.343886</td>\n",
       "      <td>-14.213665</td>\n",
       "      <td>17.301290</td>\n",
       "      <td>61.813321</td>\n",
       "      <td>118.367141</td>\n",
       "      <td>183.221865</td>\n",
       "      <td>249.319054</td>\n",
       "      <td>307.014239</td>\n",
       "      <td>346.379558</td>\n",
       "      <td>360.369182</td>\n",
       "      <td>347.130355</td>\n",
       "      <td>308.973094</td>\n",
       "      <td>252.583987</td>\n",
       "      <td>187.814469</td>\n",
       "      <td>124.789810</td>\n",
       "      <td>70.895187</td>\n",
       "      <td>28.894343</td>\n",
       "      <td>-1.227022</td>\n",
       "      <td>-21.555891</td>\n",
       "      <td>-35.419058</td>\n",
       "      <td>-46.058657</td>\n",
       "      <td>-55.581692</td>\n",
       "      <td>-64.479414</td>\n",
       "      <td>-72.558327</td>\n",
       "      <td>-79.542895</td>\n",
       "      <td>-85.247278</td>\n",
       "      <td>-89.626694</td>\n",
       "      <td>-92.800152</td>\n",
       "      <td>-95.014692</td>\n",
       "      <td>-96.535539</td>\n",
       "      <td>-97.528265</td>\n",
       "      <td>-98.079411</td>\n",
       "      <td>-98.268462</td>\n",
       "      <td>-98.182151</td>\n",
       "      <td>-97.877847</td>\n",
       "      <td>-97.381024</td>\n",
       "      <td>-96.717243</td>\n",
       "      <td>-95.922674</td>\n",
       "      <td>-95.021363</td>\n",
       "      <td>-94.015547</td>\n",
       "      <td>-92.901640</td>\n",
       "      <td>-91.685387</td>\n",
       "      <td>-90.388166</td>\n",
       "      <td>-89.011632</td>\n",
       "      <td>-87.539819</td>\n",
       "      <td>-85.974066</td>\n",
       "      <td>-84.309869</td>\n",
       "      <td>-82.534768</td>\n",
       "      <td>-80.618668</td>\n",
       "      <td>-78.546665</td>\n",
       "      <td>-76.313150</td>\n",
       "      <td>-73.882243</td>\n",
       "      <td>-71.248823</td>\n",
       "      <td>-68.420004</td>\n",
       "      <td>-65.416821</td>\n",
       "      <td>-62.280539</td>\n",
       "      <td>-59.067606</td>\n",
       "      <td>-55.863958</td>\n",
       "      <td>-52.705899</td>\n",
       "      <td>-49.611246</td>\n",
       "      <td>-46.578907</td>\n",
       "      <td>-43.560687</td>\n",
       "      <td>-40.476442</td>\n",
       "      <td>-37.218287</td>\n",
       "      <td>-33.699423</td>\n",
       "      <td>-29.858347</td>\n",
       "      <td>-25.650435</td>\n",
       "      <td>-21.043737</td>\n",
       "      <td>-16.026551</td>\n",
       "      <td>-10.591853</td>\n",
       "      <td>-4.728458</td>\n",
       "      <td>1.592870</td>\n",
       "      <td>8.417317</td>\n",
       "      <td>15.792974</td>\n",
       "      <td>23.810822</td>\n",
       "      <td>32.558446</td>\n",
       "      <td>42.093098</td>\n",
       "      <td>52.453073</td>\n",
       "      <td>63.630357</td>\n",
       "      <td>75.536966</td>\n",
       "      <td>87.945192</td>\n",
       "      <td>100.561262</td>\n",
       "      <td>113.104475</td>\n",
       "      <td>125.296269</td>\n",
       "      <td>136.883797</td>\n",
       "      <td>147.654250</td>\n",
       "      <td>157.488734</td>\n",
       "      <td>166.321037</td>\n",
       "      <td>174.048829</td>\n",
       "      <td>180.538305</td>\n",
       "      <td>185.606931</td>\n",
       "      <td>189.055862</td>\n",
       "      <td>190.660683</td>\n",
       "      <td>190.147849</td>\n",
       "      <td>187.280120</td>\n",
       "      <td>181.890234</td>\n",
       "      <td>173.972233</td>\n",
       "      <td>163.655362</td>\n",
       "      <td>151.162823</td>\n",
       "      <td>136.866472</td>\n",
       "      <td>121.242203</td>\n",
       "      <td>104.797544</td>\n",
       "      <td>87.898240</td>\n",
       "      <td>70.848631</td>\n",
       "      <td>53.972026</td>\n",
       "      <td>37.557659</td>\n",
       "      <td>21.829952</td>\n",
       "      <td>7.003641</td>\n",
       "      <td>-6.598808</td>\n",
       "      <td>-18.710091</td>\n",
       "      <td>-29.203930</td>\n",
       "      <td>-38.047099</td>\n",
       "      <td>-45.283864</td>\n",
       "      <td>-51.052402</td>\n",
       "      <td>-55.594070</td>\n",
       "      <td>-59.139776</td>\n",
       "      <td>-61.876320</td>\n",
       "      <td>-63.980914</td>\n",
       "      <td>-65.585412</td>\n",
       "      <td>-66.767905</td>\n",
       "      <td>-67.575868</td>\n",
       "      <td>-68.045530</td>\n",
       "      <td>-68.201873</td>\n",
       "      <td>-68.047691</td>\n",
       "      <td>-67.590102</td>\n",
       "      <td>-66.860754</td>\n",
       "      <td>-65.893511</td>\n",
       "      <td>-64.724872</td>\n",
       "      <td>-63.396068</td>\n",
       "      <td>-15.489147</td>\n",
       "      <td>-13.030004</td>\n",
       "      <td>-10.820615</td>\n",
       "      <td>-8.624940</td>\n",
       "      <td>-5.895799</td>\n",
       "      <td>-2.725400</td>\n",
       "      <td>-0.469552</td>\n",
       "      <td>2.266584</td>\n",
       "      <td>6.231460</td>\n",
       "      <td>10.126356</td>\n",
       "      <td>13.440836</td>\n",
       "      <td>15.515698</td>\n",
       "      <td>18.423118</td>\n",
       "      <td>20.660126</td>\n",
       "      <td>23.277018</td>\n",
       "      <td>24.091516</td>\n",
       "      <td>24.515854</td>\n",
       "      <td>25.127938</td>\n",
       "      <td>25.930316</td>\n",
       "      <td>27.018542</td>\n",
       "      <td>26.089801</td>\n",
       "      <td>24.913826</td>\n",
       "      <td>24.208905</td>\n",
       "      <td>23.207770</td>\n",
       "      <td>21.057288</td>\n",
       "      <td>18.180164</td>\n",
       "      <td>15.251634</td>\n",
       "      <td>10.320022</td>\n",
       "      <td>5.526655</td>\n",
       "      <td>-0.329275</td>\n",
       "      <td>-6.520551</td>\n",
       "      <td>-14.801481</td>\n",
       "      <td>-20.393518</td>\n",
       "      <td>-27.137130</td>\n",
       "      <td>-32.638144</td>\n",
       "      <td>-36.423865</td>\n",
       "      <td>-40.277212</td>\n",
       "      <td>-43.698567</td>\n",
       "      <td>-47.490486</td>\n",
       "      <td>-51.543123</td>\n",
       "      <td>-54.810245</td>\n",
       "      <td>-56.615152</td>\n",
       "      <td>-58.192914</td>\n",
       "      <td>-58.974315</td>\n",
       "      <td>-57.309944</td>\n",
       "      <td>-58.883707</td>\n",
       "      <td>-59.274490</td>\n",
       "      <td>-60.730837</td>\n",
       "      <td>-59.857486</td>\n",
       "      <td>-55.610578</td>\n",
       "      <td>-48.135666</td>\n",
       "      <td>-36.108145</td>\n",
       "      <td>-18.188899</td>\n",
       "      <td>10.957213</td>\n",
       "      <td>55.448484</td>\n",
       "      <td>110.667819</td>\n",
       "      <td>175.120652</td>\n",
       "      <td>235.753106</td>\n",
       "      <td>295.648090</td>\n",
       "      <td>340.506318</td>\n",
       "      <td>356.351168</td>\n",
       "      <td>341.520023</td>\n",
       "      <td>302.930429</td>\n",
       "      <td>245.382878</td>\n",
       "      <td>185.706886</td>\n",
       "      <td>122.376672</td>\n",
       "      <td>67.033858</td>\n",
       "      <td>24.924293</td>\n",
       "      <td>-4.485634</td>\n",
       "      <td>-24.920159</td>\n",
       "      <td>...</td>\n",
       "      <td>-88.547933</td>\n",
       "      <td>-89.019320</td>\n",
       "      <td>-90.127355</td>\n",
       "      <td>-92.258864</td>\n",
       "      <td>-95.856269</td>\n",
       "      <td>-101.250935</td>\n",
       "      <td>-108.468740</td>\n",
       "      <td>-117.174545</td>\n",
       "      <td>-126.866650</td>\n",
       "      <td>-136.987933</td>\n",
       "      <td>-146.759194</td>\n",
       "      <td>-155.568074</td>\n",
       "      <td>-163.024622</td>\n",
       "      <td>-168.905759</td>\n",
       "      <td>-173.044183</td>\n",
       "      <td>-175.189496</td>\n",
       "      <td>-175.147097</td>\n",
       "      <td>-172.722641</td>\n",
       "      <td>-167.666484</td>\n",
       "      <td>-159.645907</td>\n",
       "      <td>-154.558980</td>\n",
       "      <td>-159.248125</td>\n",
       "      <td>-163.096352</td>\n",
       "      <td>-166.105715</td>\n",
       "      <td>-168.306237</td>\n",
       "      <td>-169.894790</td>\n",
       "      <td>-171.068419</td>\n",
       "      <td>-172.107669</td>\n",
       "      <td>-173.237675</td>\n",
       "      <td>-174.128149</td>\n",
       "      <td>-173.698911</td>\n",
       "      <td>-169.453197</td>\n",
       "      <td>-157.755404</td>\n",
       "      <td>-134.192168</td>\n",
       "      <td>-94.738984</td>\n",
       "      <td>-38.149051</td>\n",
       "      <td>33.407889</td>\n",
       "      <td>113.623386</td>\n",
       "      <td>181.663722</td>\n",
       "      <td>235.362578</td>\n",
       "      <td>263.293168</td>\n",
       "      <td>268.110091</td>\n",
       "      <td>256.595362</td>\n",
       "      <td>222.022915</td>\n",
       "      <td>170.403166</td>\n",
       "      <td>109.778496</td>\n",
       "      <td>47.557057</td>\n",
       "      <td>-6.867491</td>\n",
       "      <td>-50.842132</td>\n",
       "      <td>-84.000763</td>\n",
       "      <td>-108.869908</td>\n",
       "      <td>-127.614806</td>\n",
       "      <td>-141.733882</td>\n",
       "      <td>-152.642114</td>\n",
       "      <td>-160.809951</td>\n",
       "      <td>-166.291106</td>\n",
       "      <td>-169.167031</td>\n",
       "      <td>-169.991375</td>\n",
       "      <td>-169.567777</td>\n",
       "      <td>-168.449881</td>\n",
       "      <td>-167.024682</td>\n",
       "      <td>-167.883440</td>\n",
       "      <td>-165.360556</td>\n",
       "      <td>-162.664542</td>\n",
       "      <td>-161.491838</td>\n",
       "      <td>-160.475140</td>\n",
       "      <td>-159.585054</td>\n",
       "      <td>-158.792256</td>\n",
       "      <td>-158.095275</td>\n",
       "      <td>-157.464944</td>\n",
       "      <td>-156.872187</td>\n",
       "      <td>-156.315797</td>\n",
       "      <td>-155.739110</td>\n",
       "      <td>-155.141124</td>\n",
       "      <td>-154.520945</td>\n",
       "      <td>-153.850016</td>\n",
       "      <td>-153.072119</td>\n",
       "      <td>-152.186716</td>\n",
       "      <td>-151.165610</td>\n",
       "      <td>-149.869621</td>\n",
       "      <td>-148.104135</td>\n",
       "      <td>-145.702434</td>\n",
       "      <td>-142.497915</td>\n",
       "      <td>-138.185193</td>\n",
       "      <td>-132.792310</td>\n",
       "      <td>-126.569616</td>\n",
       "      <td>-119.878641</td>\n",
       "      <td>-113.108747</td>\n",
       "      <td>-107.087917</td>\n",
       "      <td>-102.548624</td>\n",
       "      <td>-99.854653</td>\n",
       "      <td>-99.559947</td>\n",
       "      <td>-98.597387</td>\n",
       "      <td>-96.968555</td>\n",
       "      <td>-94.647343</td>\n",
       "      <td>-91.635502</td>\n",
       "      <td>-88.018201</td>\n",
       "      <td>-83.964022</td>\n",
       "      <td>-79.447179</td>\n",
       "      <td>-75.512136</td>\n",
       "      <td>-73.126446</td>\n",
       "      <td>-70.294428</td>\n",
       "      <td>-66.570445</td>\n",
       "      <td>-61.564506</td>\n",
       "      <td>-54.692266</td>\n",
       "      <td>-45.119472</td>\n",
       "      <td>-32.400846</td>\n",
       "      <td>-16.507860</td>\n",
       "      <td>-38.575326</td>\n",
       "      <td>-62.477736</td>\n",
       "      <td>-83.417822</td>\n",
       "      <td>-100.951659</td>\n",
       "      <td>-115.163223</td>\n",
       "      <td>-126.553296</td>\n",
       "      <td>-135.539485</td>\n",
       "      <td>-142.817345</td>\n",
       "      <td>-149.082610</td>\n",
       "      <td>-154.558980</td>\n",
       "      <td>-159.248125</td>\n",
       "      <td>-163.096352</td>\n",
       "      <td>-166.105715</td>\n",
       "      <td>-168.306237</td>\n",
       "      <td>-169.894790</td>\n",
       "      <td>-171.068419</td>\n",
       "      <td>-172.107669</td>\n",
       "      <td>-173.237675</td>\n",
       "      <td>-174.128149</td>\n",
       "      <td>-173.698911</td>\n",
       "      <td>-169.453197</td>\n",
       "      <td>-157.755404</td>\n",
       "      <td>-134.192168</td>\n",
       "      <td>-94.738984</td>\n",
       "      <td>-38.149051</td>\n",
       "      <td>33.407889</td>\n",
       "      <td>26.944514</td>\n",
       "      <td>6.637850</td>\n",
       "      <td>-13.533267</td>\n",
       "      <td>-32.927279</td>\n",
       "      <td>-50.291455</td>\n",
       "      <td>-64.817480</td>\n",
       "      <td>-76.252607</td>\n",
       "      <td>-84.538581</td>\n",
       "      <td>-89.867225</td>\n",
       "      <td>-92.736037</td>\n",
       "      <td>-94.114876</td>\n",
       "      <td>-94.640438</td>\n",
       "      <td>-94.644052</td>\n",
       "      <td>-108.869908</td>\n",
       "      <td>-127.614806</td>\n",
       "      <td>-141.733882</td>\n",
       "      <td>-152.642114</td>\n",
       "      <td>-160.809951</td>\n",
       "      <td>-166.291106</td>\n",
       "      <td>-169.167031</td>\n",
       "      <td>-169.991375</td>\n",
       "      <td>-169.567777</td>\n",
       "      <td>-168.449881</td>\n",
       "      <td>-167.024682</td>\n",
       "      <td>-165.512534</td>\n",
       "      <td>-164.022713</td>\n",
       "      <td>-162.664542</td>\n",
       "      <td>42.178220</td>\n",
       "      <td>-69.494141</td>\n",
       "      <td>-107.674375</td>\n",
       "      <td>198.932065</td>\n",
       "      <td>360.369182</td>\n",
       "      <td>815.846154</td>\n",
       "      <td>817.538462</td>\n",
       "      <td>186.931217</td>\n",
       "      <td>121.767677</td>\n",
       "      <td>117.070707</td>\n",
       "      <td>29.378788</td>\n",
       "      <td>73.708605</td>\n",
       "      <td>33.449697</td>\n",
       "      <td>-66.502132</td>\n",
       "      <td>-104.719503</td>\n",
       "      <td>196.101072</td>\n",
       "      <td>356.351168</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>823.333333</td>\n",
       "      <td>173.333333</td>\n",
       "      <td>126.666667</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>33.5</td>\n",
       "      <td>72.289157</td>\n",
       "      <td>45.665314</td>\n",
       "      <td>23.252994</td>\n",
       "      <td>18.778421</td>\n",
       "      <td>25.495103</td>\n",
       "      <td>46.431980</td>\n",
       "      <td>65.387813</td>\n",
       "      <td>96.618515</td>\n",
       "      <td>72.145995</td>\n",
       "      <td>42.633114</td>\n",
       "      <td>75.088086</td>\n",
       "      <td>10.046506</td>\n",
       "      <td>8.510272</td>\n",
       "      <td>273.236801</td>\n",
       "      <td>-38.207580</td>\n",
       "      <td>-63.517605</td>\n",
       "      <td>298.723811</td>\n",
       "      <td>535.188836</td>\n",
       "      <td>853.333333</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>413.333333</td>\n",
       "      <td>333.333333</td>\n",
       "      <td>376.666667</td>\n",
       "      <td>44.0</td>\n",
       "      <td>185.567010</td>\n",
       "      <td>-41.018732</td>\n",
       "      <td>-175.147097</td>\n",
       "      <td>-169.567777</td>\n",
       "      <td>136.528583</td>\n",
       "      <td>268.110091</td>\n",
       "      <td>323.333333</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.312500</td>\n",
       "      <td>815.846154</td>\n",
       "      <td>65.896675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.209942</td>\n",
       "      <td>64.223760</td>\n",
       "      <td>0.080771</td>\n",
       "      <td>0.078703</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>14.826</td>\n",
       "      <td>0.017863</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>3.823529</td>\n",
       "      <td>523.4375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>1.409581</td>\n",
       "      <td>0.568152</td>\n",
       "      <td>0.403065</td>\n",
       "      <td>-5.364064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.753309</td>\n",
       "      <td>-0.242555</td>\n",
       "      <td>1.452685</td>\n",
       "      <td>3.257832</td>\n",
       "      <td>5.084793</td>\n",
       "      <td>6.856565</td>\n",
       "      <td>8.538980</td>\n",
       "      <td>10.120082</td>\n",
       "      <td>11.629965</td>\n",
       "      <td>13.121732</td>\n",
       "      <td>14.672293</td>\n",
       "      <td>16.334753</td>\n",
       "      <td>18.086824</td>\n",
       "      <td>19.897503</td>\n",
       "      <td>21.708818</td>\n",
       "      <td>23.449324</td>\n",
       "      <td>25.021414</td>\n",
       "      <td>26.338618</td>\n",
       "      <td>27.360529</td>\n",
       "      <td>28.045984</td>\n",
       "      <td>28.352265</td>\n",
       "      <td>28.204947</td>\n",
       "      <td>27.537580</td>\n",
       "      <td>26.290099</td>\n",
       "      <td>24.404062</td>\n",
       "      <td>21.841696</td>\n",
       "      <td>18.586689</td>\n",
       "      <td>14.691006</td>\n",
       "      <td>10.262195</td>\n",
       "      <td>5.426866</td>\n",
       "      <td>0.328308</td>\n",
       "      <td>-4.883041</td>\n",
       "      <td>-10.036107</td>\n",
       "      <td>-14.982046</td>\n",
       "      <td>-19.622026</td>\n",
       "      <td>-23.880251</td>\n",
       "      <td>-27.706346</td>\n",
       "      <td>-31.097585</td>\n",
       "      <td>-34.095723</td>\n",
       "      <td>-36.785412</td>\n",
       "      <td>-39.259284</td>\n",
       "      <td>-41.603668</td>\n",
       "      <td>-43.887478</td>\n",
       "      <td>-46.131266</td>\n",
       "      <td>-48.310392</td>\n",
       "      <td>-50.288354</td>\n",
       "      <td>-51.815995</td>\n",
       "      <td>-52.530699</td>\n",
       "      <td>-51.945275</td>\n",
       "      <td>-49.416204</td>\n",
       "      <td>-44.089653</td>\n",
       "      <td>-35.117344</td>\n",
       "      <td>-21.790658</td>\n",
       "      <td>-3.656500</td>\n",
       "      <td>19.260505</td>\n",
       "      <td>46.150882</td>\n",
       "      <td>75.344927</td>\n",
       "      <td>104.435743</td>\n",
       "      <td>130.190362</td>\n",
       "      <td>148.560079</td>\n",
       "      <td>155.609025</td>\n",
       "      <td>148.581630</td>\n",
       "      <td>126.583574</td>\n",
       "      <td>90.834151</td>\n",
       "      <td>44.890066</td>\n",
       "      <td>-5.244272</td>\n",
       "      <td>-52.720740</td>\n",
       "      <td>-91.638008</td>\n",
       "      <td>-118.340765</td>\n",
       "      <td>-131.603079</td>\n",
       "      <td>-132.383179</td>\n",
       "      <td>-123.702039</td>\n",
       "      <td>-109.549739</td>\n",
       "      <td>-93.408490</td>\n",
       "      <td>-77.708192</td>\n",
       "      <td>-63.882780</td>\n",
       "      <td>-52.613881</td>\n",
       "      <td>-43.970487</td>\n",
       "      <td>-37.506574</td>\n",
       "      <td>-32.830940</td>\n",
       "      <td>-29.532595</td>\n",
       "      <td>-27.187109</td>\n",
       "      <td>-25.383592</td>\n",
       "      <td>-23.743740</td>\n",
       "      <td>-22.077385</td>\n",
       "      <td>-20.285667</td>\n",
       "      <td>-18.326909</td>\n",
       "      <td>-16.199944</td>\n",
       "      <td>-13.930626</td>\n",
       "      <td>-11.563092</td>\n",
       "      <td>-9.099451</td>\n",
       "      <td>-6.510889</td>\n",
       "      <td>-3.826561</td>\n",
       "      <td>-1.134383</td>\n",
       "      <td>1.449127</td>\n",
       "      <td>3.778846</td>\n",
       "      <td>5.706445</td>\n",
       "      <td>7.089908</td>\n",
       "      <td>7.907028</td>\n",
       "      <td>8.297462</td>\n",
       "      <td>8.465114</td>\n",
       "      <td>8.689247</td>\n",
       "      <td>9.210981</td>\n",
       "      <td>10.215838</td>\n",
       "      <td>11.809138</td>\n",
       "      <td>13.955674</td>\n",
       "      <td>16.563843</td>\n",
       "      <td>19.503109</td>\n",
       "      <td>22.692091</td>\n",
       "      <td>26.074763</td>\n",
       "      <td>29.604578</td>\n",
       "      <td>33.285741</td>\n",
       "      <td>37.157341</td>\n",
       "      <td>41.257637</td>\n",
       "      <td>45.575654</td>\n",
       "      <td>50.092454</td>\n",
       "      <td>54.787490</td>\n",
       "      <td>59.633854</td>\n",
       "      <td>64.585582</td>\n",
       "      <td>69.565754</td>\n",
       "      <td>74.507773</td>\n",
       "      <td>79.344262</td>\n",
       "      <td>83.976907</td>\n",
       "      <td>88.309008</td>\n",
       "      <td>92.243897</td>\n",
       "      <td>95.723038</td>\n",
       "      <td>98.698255</td>\n",
       "      <td>101.119834</td>\n",
       "      <td>102.975412</td>\n",
       "      <td>104.231255</td>\n",
       "      <td>104.829082</td>\n",
       "      <td>104.681306</td>\n",
       "      <td>103.682146</td>\n",
       "      <td>101.734609</td>\n",
       "      <td>98.725887</td>\n",
       "      <td>94.561477</td>\n",
       "      <td>89.215975</td>\n",
       "      <td>82.731479</td>\n",
       "      <td>75.201715</td>\n",
       "      <td>66.716467</td>\n",
       "      <td>57.402053</td>\n",
       "      <td>47.455438</td>\n",
       "      <td>37.072799</td>\n",
       "      <td>26.420948</td>\n",
       "      <td>15.652397</td>\n",
       "      <td>5.007733</td>\n",
       "      <td>-5.262165</td>\n",
       "      <td>-14.965980</td>\n",
       "      <td>-23.932277</td>\n",
       "      <td>-32.010308</td>\n",
       "      <td>-39.052550</td>\n",
       "      <td>-45.039321</td>\n",
       "      <td>-50.000205</td>\n",
       "      <td>-53.988665</td>\n",
       "      <td>-57.112989</td>\n",
       "      <td>-59.480745</td>\n",
       "      <td>-61.213059</td>\n",
       "      <td>-62.364459</td>\n",
       "      <td>-62.983984</td>\n",
       "      <td>-63.112007</td>\n",
       "      <td>-62.765944</td>\n",
       "      <td>-61.983109</td>\n",
       "      <td>-60.815151</td>\n",
       "      <td>-59.347888</td>\n",
       "      <td>-57.664004</td>\n",
       "      <td>-55.851767</td>\n",
       "      <td>-53.994705</td>\n",
       "      <td>-52.162079</td>\n",
       "      <td>-50.396969</td>\n",
       "      <td>-48.718647</td>\n",
       "      <td>-47.151145</td>\n",
       "      <td>-45.705784</td>\n",
       "      <td>-44.396253</td>\n",
       "      <td>-43.188601</td>\n",
       "      <td>-42.066310</td>\n",
       "      <td>-41.028706</td>\n",
       "      <td>-40.076671</td>\n",
       "      <td>-39.219778</td>\n",
       "      <td>-38.435817</td>\n",
       "      <td>-37.742220</td>\n",
       "      <td>-1.219333</td>\n",
       "      <td>-2.887117</td>\n",
       "      <td>-3.042428</td>\n",
       "      <td>-0.180479</td>\n",
       "      <td>-0.035532</td>\n",
       "      <td>1.125449</td>\n",
       "      <td>3.783738</td>\n",
       "      <td>7.898919</td>\n",
       "      <td>11.031366</td>\n",
       "      <td>14.846134</td>\n",
       "      <td>16.350363</td>\n",
       "      <td>19.309462</td>\n",
       "      <td>23.618644</td>\n",
       "      <td>24.622350</td>\n",
       "      <td>23.882625</td>\n",
       "      <td>25.319072</td>\n",
       "      <td>27.576786</td>\n",
       "      <td>29.786530</td>\n",
       "      <td>28.309271</td>\n",
       "      <td>27.088866</td>\n",
       "      <td>25.951592</td>\n",
       "      <td>28.437887</td>\n",
       "      <td>26.275017</td>\n",
       "      <td>26.474506</td>\n",
       "      <td>29.493239</td>\n",
       "      <td>30.233601</td>\n",
       "      <td>26.932193</td>\n",
       "      <td>20.750344</td>\n",
       "      <td>14.169941</td>\n",
       "      <td>7.495982</td>\n",
       "      <td>4.068793</td>\n",
       "      <td>1.609599</td>\n",
       "      <td>-2.755134</td>\n",
       "      <td>-8.925125</td>\n",
       "      <td>-14.679182</td>\n",
       "      <td>-21.595700</td>\n",
       "      <td>-24.854458</td>\n",
       "      <td>-29.247049</td>\n",
       "      <td>-30.952465</td>\n",
       "      <td>-34.040928</td>\n",
       "      <td>-36.345614</td>\n",
       "      <td>-38.808245</td>\n",
       "      <td>-41.905718</td>\n",
       "      <td>-45.985519</td>\n",
       "      <td>-49.260584</td>\n",
       "      <td>-51.605338</td>\n",
       "      <td>-54.501382</td>\n",
       "      <td>-54.923608</td>\n",
       "      <td>-55.732643</td>\n",
       "      <td>-49.939888</td>\n",
       "      <td>-46.619433</td>\n",
       "      <td>-35.695846</td>\n",
       "      <td>-24.709805</td>\n",
       "      <td>-11.435534</td>\n",
       "      <td>11.168290</td>\n",
       "      <td>34.970849</td>\n",
       "      <td>67.112959</td>\n",
       "      <td>94.601651</td>\n",
       "      <td>117.972197</td>\n",
       "      <td>136.560382</td>\n",
       "      <td>141.058357</td>\n",
       "      <td>133.278022</td>\n",
       "      <td>115.232093</td>\n",
       "      <td>78.198366</td>\n",
       "      <td>35.353863</td>\n",
       "      <td>-13.861311</td>\n",
       "      <td>-60.850271</td>\n",
       "      <td>-99.608253</td>\n",
       "      <td>-122.481351</td>\n",
       "      <td>-135.805903</td>\n",
       "      <td>...</td>\n",
       "      <td>-148.102136</td>\n",
       "      <td>-148.435120</td>\n",
       "      <td>-148.071567</td>\n",
       "      <td>-147.175232</td>\n",
       "      <td>-145.993129</td>\n",
       "      <td>-144.716644</td>\n",
       "      <td>-143.564875</td>\n",
       "      <td>-142.784641</td>\n",
       "      <td>-142.456042</td>\n",
       "      <td>-142.659135</td>\n",
       "      <td>-143.390609</td>\n",
       "      <td>-144.647124</td>\n",
       "      <td>-146.369767</td>\n",
       "      <td>-148.416286</td>\n",
       "      <td>-150.699990</td>\n",
       "      <td>-153.106431</td>\n",
       "      <td>-155.576752</td>\n",
       "      <td>-158.024374</td>\n",
       "      <td>-160.362790</td>\n",
       "      <td>-162.533368</td>\n",
       "      <td>-164.422034</td>\n",
       "      <td>-165.748181</td>\n",
       "      <td>-165.786908</td>\n",
       "      <td>-163.480141</td>\n",
       "      <td>-157.908862</td>\n",
       "      <td>-148.182000</td>\n",
       "      <td>-133.436420</td>\n",
       "      <td>-113.114693</td>\n",
       "      <td>-104.015477</td>\n",
       "      <td>-107.492194</td>\n",
       "      <td>-109.214349</td>\n",
       "      <td>-108.071708</td>\n",
       "      <td>-102.898443</td>\n",
       "      <td>-92.250895</td>\n",
       "      <td>-74.685339</td>\n",
       "      <td>-49.230191</td>\n",
       "      <td>-16.608211</td>\n",
       "      <td>21.096845</td>\n",
       "      <td>45.353370</td>\n",
       "      <td>64.450345</td>\n",
       "      <td>76.777817</td>\n",
       "      <td>79.225118</td>\n",
       "      <td>69.709449</td>\n",
       "      <td>48.398096</td>\n",
       "      <td>16.819536</td>\n",
       "      <td>-21.553241</td>\n",
       "      <td>-76.593105</td>\n",
       "      <td>-130.380987</td>\n",
       "      <td>-171.580882</td>\n",
       "      <td>-196.858438</td>\n",
       "      <td>-208.845463</td>\n",
       "      <td>-218.906818</td>\n",
       "      <td>-213.228019</td>\n",
       "      <td>-195.891290</td>\n",
       "      <td>-182.338511</td>\n",
       "      <td>-173.722071</td>\n",
       "      <td>-165.882849</td>\n",
       "      <td>-159.015529</td>\n",
       "      <td>-152.731567</td>\n",
       "      <td>-146.531412</td>\n",
       "      <td>-139.804490</td>\n",
       "      <td>-132.301418</td>\n",
       "      <td>-123.911774</td>\n",
       "      <td>-114.691859</td>\n",
       "      <td>-104.920245</td>\n",
       "      <td>-99.423865</td>\n",
       "      <td>-97.547836</td>\n",
       "      <td>-95.313473</td>\n",
       "      <td>-92.386114</td>\n",
       "      <td>-88.431242</td>\n",
       "      <td>-83.253366</td>\n",
       "      <td>-76.712689</td>\n",
       "      <td>-68.475097</td>\n",
       "      <td>-58.262159</td>\n",
       "      <td>-50.800350</td>\n",
       "      <td>-51.643810</td>\n",
       "      <td>-52.150019</td>\n",
       "      <td>-52.236058</td>\n",
       "      <td>-54.506096</td>\n",
       "      <td>-59.316539</td>\n",
       "      <td>-62.564915</td>\n",
       "      <td>-64.446109</td>\n",
       "      <td>-65.571776</td>\n",
       "      <td>-66.137005</td>\n",
       "      <td>-68.294204</td>\n",
       "      <td>-85.047368</td>\n",
       "      <td>-87.188405</td>\n",
       "      <td>-79.661667</td>\n",
       "      <td>-67.967300</td>\n",
       "      <td>-58.446898</td>\n",
       "      <td>-55.365442</td>\n",
       "      <td>-51.232145</td>\n",
       "      <td>-45.770655</td>\n",
       "      <td>-38.954670</td>\n",
       "      <td>-34.484791</td>\n",
       "      <td>-36.612438</td>\n",
       "      <td>-39.859366</td>\n",
       "      <td>-43.838695</td>\n",
       "      <td>-48.080378</td>\n",
       "      <td>-51.975631</td>\n",
       "      <td>-55.165819</td>\n",
       "      <td>-57.486888</td>\n",
       "      <td>-58.913811</td>\n",
       "      <td>-59.505018</td>\n",
       "      <td>-59.402393</td>\n",
       "      <td>-58.886826</td>\n",
       "      <td>-58.155982</td>\n",
       "      <td>-57.324295</td>\n",
       "      <td>-56.450749</td>\n",
       "      <td>-55.511086</td>\n",
       "      <td>-54.481144</td>\n",
       "      <td>-53.309075</td>\n",
       "      <td>-51.943117</td>\n",
       "      <td>-50.359373</td>\n",
       "      <td>-48.534031</td>\n",
       "      <td>-46.471138</td>\n",
       "      <td>-44.202600</td>\n",
       "      <td>-41.760400</td>\n",
       "      <td>-39.148818</td>\n",
       "      <td>-36.372204</td>\n",
       "      <td>-33.434976</td>\n",
       "      <td>-30.341614</td>\n",
       "      <td>-27.041100</td>\n",
       "      <td>-27.289439</td>\n",
       "      <td>-44.063488</td>\n",
       "      <td>-57.971507</td>\n",
       "      <td>-68.596745</td>\n",
       "      <td>-75.996953</td>\n",
       "      <td>-85.460687</td>\n",
       "      <td>-93.506936</td>\n",
       "      <td>-100.246186</td>\n",
       "      <td>-105.816716</td>\n",
       "      <td>-110.384600</td>\n",
       "      <td>-114.338159</td>\n",
       "      <td>-117.871305</td>\n",
       "      <td>-121.039092</td>\n",
       "      <td>-123.841053</td>\n",
       "      <td>-126.276759</td>\n",
       "      <td>-128.429150</td>\n",
       "      <td>-130.325643</td>\n",
       "      <td>-131.938135</td>\n",
       "      <td>-133.210772</td>\n",
       "      <td>-134.087729</td>\n",
       "      <td>-134.907924</td>\n",
       "      <td>-135.454496</td>\n",
       "      <td>-134.930457</td>\n",
       "      <td>-133.115240</td>\n",
       "      <td>-130.418238</td>\n",
       "      <td>-127.916551</td>\n",
       "      <td>-131.260199</td>\n",
       "      <td>-135.290131</td>\n",
       "      <td>-138.993104</td>\n",
       "      <td>-142.396642</td>\n",
       "      <td>-145.639307</td>\n",
       "      <td>-148.804036</td>\n",
       "      <td>-151.918142</td>\n",
       "      <td>-154.981095</td>\n",
       "      <td>-157.964530</td>\n",
       "      <td>-160.951137</td>\n",
       "      <td>-163.940223</td>\n",
       "      <td>-166.931056</td>\n",
       "      <td>45.600836</td>\n",
       "      <td>-58.337274</td>\n",
       "      <td>-135.289086</td>\n",
       "      <td>116.187169</td>\n",
       "      <td>155.098157</td>\n",
       "      <td>789.047619</td>\n",
       "      <td>811.568627</td>\n",
       "      <td>167.333333</td>\n",
       "      <td>132.190476</td>\n",
       "      <td>135.238095</td>\n",
       "      <td>32.142857</td>\n",
       "      <td>73.846890</td>\n",
       "      <td>43.067625</td>\n",
       "      <td>-58.336062</td>\n",
       "      <td>-137.640020</td>\n",
       "      <td>113.876522</td>\n",
       "      <td>140.949618</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>786.666667</td>\n",
       "      <td>173.333333</td>\n",
       "      <td>123.333333</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>71.713147</td>\n",
       "      <td>36.216761</td>\n",
       "      <td>27.299799</td>\n",
       "      <td>62.163862</td>\n",
       "      <td>54.910037</td>\n",
       "      <td>40.242121</td>\n",
       "      <td>51.551001</td>\n",
       "      <td>174.405124</td>\n",
       "      <td>31.860808</td>\n",
       "      <td>45.662272</td>\n",
       "      <td>76.562188</td>\n",
       "      <td>8.986157</td>\n",
       "      <td>4.067595</td>\n",
       "      <td>138.536977</td>\n",
       "      <td>-10.137490</td>\n",
       "      <td>159.419397</td>\n",
       "      <td>341.411772</td>\n",
       "      <td>231.478680</td>\n",
       "      <td>863.333333</td>\n",
       "      <td>1590.000000</td>\n",
       "      <td>226.666667</td>\n",
       "      <td>286.666667</td>\n",
       "      <td>343.333333</td>\n",
       "      <td>51.0</td>\n",
       "      <td>102.857143</td>\n",
       "      <td>-94.812612</td>\n",
       "      <td>-165.786908</td>\n",
       "      <td>-218.906818</td>\n",
       "      <td>-7.091954</td>\n",
       "      <td>79.225118</td>\n",
       "      <td>583.333333</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>103.333333</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>69.498069</td>\n",
       "      <td>789.047619</td>\n",
       "      <td>52.303610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.209050</td>\n",
       "      <td>56.032151</td>\n",
       "      <td>0.066287</td>\n",
       "      <td>0.069969</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>44.478</td>\n",
       "      <td>0.055597</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>68.571429</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>226.5625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024982</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.699037</td>\n",
       "      <td>-3.689603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-15.740629</td>\n",
       "      <td>-13.864271</td>\n",
       "      <td>-11.691737</td>\n",
       "      <td>-9.261015</td>\n",
       "      <td>-6.574892</td>\n",
       "      <td>-3.650031</td>\n",
       "      <td>-0.504923</td>\n",
       "      <td>2.855850</td>\n",
       "      <td>6.435141</td>\n",
       "      <td>10.251569</td>\n",
       "      <td>14.259899</td>\n",
       "      <td>18.406591</td>\n",
       "      <td>22.605733</td>\n",
       "      <td>26.734411</td>\n",
       "      <td>30.600296</td>\n",
       "      <td>33.957392</td>\n",
       "      <td>36.579173</td>\n",
       "      <td>38.209511</td>\n",
       "      <td>38.609889</td>\n",
       "      <td>37.569581</td>\n",
       "      <td>34.983425</td>\n",
       "      <td>30.876804</td>\n",
       "      <td>25.388980</td>\n",
       "      <td>18.786963</td>\n",
       "      <td>11.434943</td>\n",
       "      <td>3.763724</td>\n",
       "      <td>-3.796887</td>\n",
       "      <td>-10.933831</td>\n",
       "      <td>-17.445261</td>\n",
       "      <td>-23.219257</td>\n",
       "      <td>-28.240323</td>\n",
       "      <td>-32.569955</td>\n",
       "      <td>-36.326282</td>\n",
       "      <td>-39.597041</td>\n",
       "      <td>-42.451623</td>\n",
       "      <td>-44.964229</td>\n",
       "      <td>-47.192584</td>\n",
       "      <td>-49.179789</td>\n",
       "      <td>-50.943216</td>\n",
       "      <td>-52.488397</td>\n",
       "      <td>-53.810874</td>\n",
       "      <td>-54.886016</td>\n",
       "      <td>-55.676414</td>\n",
       "      <td>-56.121689</td>\n",
       "      <td>-56.150521</td>\n",
       "      <td>-55.682482</td>\n",
       "      <td>-54.570613</td>\n",
       "      <td>-52.469919</td>\n",
       "      <td>-48.629936</td>\n",
       "      <td>-41.671550</td>\n",
       "      <td>-29.430482</td>\n",
       "      <td>-8.805396</td>\n",
       "      <td>24.099558</td>\n",
       "      <td>73.359048</td>\n",
       "      <td>142.017361</td>\n",
       "      <td>230.231970</td>\n",
       "      <td>333.808784</td>\n",
       "      <td>443.287382</td>\n",
       "      <td>543.791992</td>\n",
       "      <td>617.421342</td>\n",
       "      <td>647.143129</td>\n",
       "      <td>621.881064</td>\n",
       "      <td>540.280601</td>\n",
       "      <td>411.398716</td>\n",
       "      <td>254.311266</td>\n",
       "      <td>93.549033</td>\n",
       "      <td>-47.204205</td>\n",
       "      <td>-151.228411</td>\n",
       "      <td>-212.092372</td>\n",
       "      <td>-232.400981</td>\n",
       "      <td>-222.175855</td>\n",
       "      <td>-195.052599</td>\n",
       "      <td>-163.580845</td>\n",
       "      <td>-135.964085</td>\n",
       "      <td>-115.610621</td>\n",
       "      <td>-102.803016</td>\n",
       "      <td>-95.820323</td>\n",
       "      <td>-92.112165</td>\n",
       "      <td>-89.606138</td>\n",
       "      <td>-87.341151</td>\n",
       "      <td>-85.039638</td>\n",
       "      <td>-82.624227</td>\n",
       "      <td>-80.074213</td>\n",
       "      <td>-77.401477</td>\n",
       "      <td>-74.648628</td>\n",
       "      <td>-71.838068</td>\n",
       "      <td>-68.996978</td>\n",
       "      <td>-66.151754</td>\n",
       "      <td>-63.334476</td>\n",
       "      <td>-60.571786</td>\n",
       "      <td>-57.870062</td>\n",
       "      <td>-55.242254</td>\n",
       "      <td>-52.685655</td>\n",
       "      <td>-50.179111</td>\n",
       "      <td>-47.686710</td>\n",
       "      <td>-45.184629</td>\n",
       "      <td>-42.650007</td>\n",
       "      <td>-40.070198</td>\n",
       "      <td>-37.447390</td>\n",
       "      <td>-34.789337</td>\n",
       "      <td>-32.127868</td>\n",
       "      <td>-29.467029</td>\n",
       "      <td>-26.836778</td>\n",
       "      <td>-24.259648</td>\n",
       "      <td>-21.744252</td>\n",
       "      <td>-19.302879</td>\n",
       "      <td>-16.922780</td>\n",
       "      <td>-14.630050</td>\n",
       "      <td>-12.416485</td>\n",
       "      <td>-10.267348</td>\n",
       "      <td>-8.177109</td>\n",
       "      <td>-6.138337</td>\n",
       "      <td>-4.167618</td>\n",
       "      <td>-2.254633</td>\n",
       "      <td>-0.403820</td>\n",
       "      <td>1.372103</td>\n",
       "      <td>3.071582</td>\n",
       "      <td>4.695896</td>\n",
       "      <td>6.257484</td>\n",
       "      <td>7.761428</td>\n",
       "      <td>9.211002</td>\n",
       "      <td>10.603969</td>\n",
       "      <td>11.939981</td>\n",
       "      <td>13.208542</td>\n",
       "      <td>14.415850</td>\n",
       "      <td>15.558875</td>\n",
       "      <td>16.630901</td>\n",
       "      <td>17.648377</td>\n",
       "      <td>18.622209</td>\n",
       "      <td>19.574417</td>\n",
       "      <td>20.505725</td>\n",
       "      <td>21.431668</td>\n",
       "      <td>22.377022</td>\n",
       "      <td>23.342474</td>\n",
       "      <td>24.328683</td>\n",
       "      <td>25.341832</td>\n",
       "      <td>26.395472</td>\n",
       "      <td>27.504038</td>\n",
       "      <td>28.670806</td>\n",
       "      <td>29.918447</td>\n",
       "      <td>31.252918</td>\n",
       "      <td>32.670864</td>\n",
       "      <td>34.166109</td>\n",
       "      <td>35.737994</td>\n",
       "      <td>37.391374</td>\n",
       "      <td>39.101451</td>\n",
       "      <td>40.852659</td>\n",
       "      <td>42.609047</td>\n",
       "      <td>44.335579</td>\n",
       "      <td>45.973142</td>\n",
       "      <td>47.471885</td>\n",
       "      <td>48.801412</td>\n",
       "      <td>49.936898</td>\n",
       "      <td>50.880393</td>\n",
       "      <td>51.610832</td>\n",
       "      <td>52.097922</td>\n",
       "      <td>52.292901</td>\n",
       "      <td>52.134091</td>\n",
       "      <td>51.542278</td>\n",
       "      <td>50.424426</td>\n",
       "      <td>48.694047</td>\n",
       "      <td>46.277693</td>\n",
       "      <td>43.131626</td>\n",
       "      <td>39.250156</td>\n",
       "      <td>34.676756</td>\n",
       "      <td>29.482770</td>\n",
       "      <td>23.785931</td>\n",
       "      <td>17.742950</td>\n",
       "      <td>11.493966</td>\n",
       "      <td>5.156976</td>\n",
       "      <td>-1.192533</td>\n",
       "      <td>-7.492895</td>\n",
       "      <td>-13.690718</td>\n",
       "      <td>-19.744595</td>\n",
       "      <td>-25.604752</td>\n",
       "      <td>-31.211202</td>\n",
       "      <td>-36.490062</td>\n",
       "      <td>-41.363742</td>\n",
       "      <td>-45.776887</td>\n",
       "      <td>-49.701018</td>\n",
       "      <td>-12.624124</td>\n",
       "      <td>-11.869465</td>\n",
       "      <td>-10.778435</td>\n",
       "      <td>-7.773356</td>\n",
       "      <td>-5.155552</td>\n",
       "      <td>-2.293272</td>\n",
       "      <td>-0.425352</td>\n",
       "      <td>3.554757</td>\n",
       "      <td>8.073808</td>\n",
       "      <td>12.018306</td>\n",
       "      <td>16.318128</td>\n",
       "      <td>20.160432</td>\n",
       "      <td>25.424818</td>\n",
       "      <td>29.810309</td>\n",
       "      <td>33.939505</td>\n",
       "      <td>37.288502</td>\n",
       "      <td>39.171516</td>\n",
       "      <td>40.025286</td>\n",
       "      <td>39.581442</td>\n",
       "      <td>38.437253</td>\n",
       "      <td>35.860205</td>\n",
       "      <td>31.740109</td>\n",
       "      <td>26.133421</td>\n",
       "      <td>19.179903</td>\n",
       "      <td>10.977090</td>\n",
       "      <td>1.690469</td>\n",
       "      <td>-7.581131</td>\n",
       "      <td>-13.527236</td>\n",
       "      <td>-18.859527</td>\n",
       "      <td>-24.120911</td>\n",
       "      <td>-29.184199</td>\n",
       "      <td>-33.828832</td>\n",
       "      <td>-37.510691</td>\n",
       "      <td>-40.611693</td>\n",
       "      <td>-44.171125</td>\n",
       "      <td>-47.754708</td>\n",
       "      <td>-50.296313</td>\n",
       "      <td>-52.814126</td>\n",
       "      <td>-54.693051</td>\n",
       "      <td>-56.737631</td>\n",
       "      <td>-58.009052</td>\n",
       "      <td>-58.733156</td>\n",
       "      <td>-59.667189</td>\n",
       "      <td>-60.116707</td>\n",
       "      <td>-59.545000</td>\n",
       "      <td>-58.676832</td>\n",
       "      <td>-57.325058</td>\n",
       "      <td>-55.091000</td>\n",
       "      <td>-51.230835</td>\n",
       "      <td>-45.776798</td>\n",
       "      <td>-31.882237</td>\n",
       "      <td>-11.905830</td>\n",
       "      <td>22.877433</td>\n",
       "      <td>69.255212</td>\n",
       "      <td>137.888729</td>\n",
       "      <td>225.356285</td>\n",
       "      <td>326.725024</td>\n",
       "      <td>439.109955</td>\n",
       "      <td>542.146524</td>\n",
       "      <td>617.828077</td>\n",
       "      <td>645.257053</td>\n",
       "      <td>621.640783</td>\n",
       "      <td>542.443148</td>\n",
       "      <td>412.808402</td>\n",
       "      <td>258.607114</td>\n",
       "      <td>99.046169</td>\n",
       "      <td>-42.777537</td>\n",
       "      <td>-152.298608</td>\n",
       "      <td>-216.774954</td>\n",
       "      <td>-237.856332</td>\n",
       "      <td>...</td>\n",
       "      <td>-49.258874</td>\n",
       "      <td>-51.862350</td>\n",
       "      <td>-55.717751</td>\n",
       "      <td>-60.458831</td>\n",
       "      <td>-65.747069</td>\n",
       "      <td>-71.188346</td>\n",
       "      <td>-76.499629</td>\n",
       "      <td>-81.453426</td>\n",
       "      <td>-85.877801</td>\n",
       "      <td>-89.600835</td>\n",
       "      <td>-92.506193</td>\n",
       "      <td>-94.588698</td>\n",
       "      <td>-95.954339</td>\n",
       "      <td>-96.709179</td>\n",
       "      <td>-96.903808</td>\n",
       "      <td>-96.588912</td>\n",
       "      <td>-95.926395</td>\n",
       "      <td>-94.939389</td>\n",
       "      <td>-93.567817</td>\n",
       "      <td>-91.835068</td>\n",
       "      <td>-89.848001</td>\n",
       "      <td>-87.769175</td>\n",
       "      <td>-85.566848</td>\n",
       "      <td>-83.181642</td>\n",
       "      <td>-82.434643</td>\n",
       "      <td>-83.447288</td>\n",
       "      <td>-83.885479</td>\n",
       "      <td>-83.390227</td>\n",
       "      <td>-81.269346</td>\n",
       "      <td>-76.330754</td>\n",
       "      <td>-66.632438</td>\n",
       "      <td>-49.260197</td>\n",
       "      <td>-33.407509</td>\n",
       "      <td>-7.343445</td>\n",
       "      <td>34.055886</td>\n",
       "      <td>93.688717</td>\n",
       "      <td>171.635383</td>\n",
       "      <td>267.982913</td>\n",
       "      <td>375.424092</td>\n",
       "      <td>481.152093</td>\n",
       "      <td>565.983967</td>\n",
       "      <td>600.110386</td>\n",
       "      <td>555.225303</td>\n",
       "      <td>446.975084</td>\n",
       "      <td>301.920451</td>\n",
       "      <td>142.840979</td>\n",
       "      <td>-5.483496</td>\n",
       "      <td>-129.118325</td>\n",
       "      <td>-216.127621</td>\n",
       "      <td>-258.825272</td>\n",
       "      <td>-264.125043</td>\n",
       "      <td>-252.047729</td>\n",
       "      <td>-225.481853</td>\n",
       "      <td>-195.123063</td>\n",
       "      <td>-168.765509</td>\n",
       "      <td>-149.308818</td>\n",
       "      <td>-137.736904</td>\n",
       "      <td>-131.929044</td>\n",
       "      <td>-131.678389</td>\n",
       "      <td>-132.410667</td>\n",
       "      <td>-132.846320</td>\n",
       "      <td>-132.540209</td>\n",
       "      <td>-131.436292</td>\n",
       "      <td>-129.645397</td>\n",
       "      <td>-127.334104</td>\n",
       "      <td>-124.752514</td>\n",
       "      <td>-121.984242</td>\n",
       "      <td>-119.113074</td>\n",
       "      <td>-116.222951</td>\n",
       "      <td>-113.397960</td>\n",
       "      <td>-110.666769</td>\n",
       "      <td>-107.974831</td>\n",
       "      <td>-105.378817</td>\n",
       "      <td>-102.852163</td>\n",
       "      <td>-100.312829</td>\n",
       "      <td>-97.678844</td>\n",
       "      <td>-94.868297</td>\n",
       "      <td>-91.827097</td>\n",
       "      <td>-88.646292</td>\n",
       "      <td>-86.273780</td>\n",
       "      <td>-83.888295</td>\n",
       "      <td>-81.463796</td>\n",
       "      <td>-78.974204</td>\n",
       "      <td>-76.337838</td>\n",
       "      <td>-73.528520</td>\n",
       "      <td>-70.520010</td>\n",
       "      <td>-67.369340</td>\n",
       "      <td>-64.105691</td>\n",
       "      <td>-60.785953</td>\n",
       "      <td>-57.466935</td>\n",
       "      <td>-54.205371</td>\n",
       "      <td>-51.057910</td>\n",
       "      <td>-48.025567</td>\n",
       "      <td>-45.137050</td>\n",
       "      <td>-42.337649</td>\n",
       "      <td>-39.628132</td>\n",
       "      <td>-36.981402</td>\n",
       "      <td>-34.342511</td>\n",
       "      <td>-31.711983</td>\n",
       "      <td>-29.241329</td>\n",
       "      <td>-27.077279</td>\n",
       "      <td>-26.860028</td>\n",
       "      <td>-26.819392</td>\n",
       "      <td>-26.955640</td>\n",
       "      <td>-27.185669</td>\n",
       "      <td>-27.481906</td>\n",
       "      <td>-27.761196</td>\n",
       "      <td>-27.912589</td>\n",
       "      <td>-27.797346</td>\n",
       "      <td>-27.332278</td>\n",
       "      <td>-26.461974</td>\n",
       "      <td>-25.131027</td>\n",
       "      <td>-23.339598</td>\n",
       "      <td>-21.115640</td>\n",
       "      <td>-18.570461</td>\n",
       "      <td>-15.787618</td>\n",
       "      <td>-12.822919</td>\n",
       "      <td>-9.759984</td>\n",
       "      <td>-6.626917</td>\n",
       "      <td>-3.479638</td>\n",
       "      <td>-0.290772</td>\n",
       "      <td>2.967015</td>\n",
       "      <td>6.265465</td>\n",
       "      <td>9.576287</td>\n",
       "      <td>10.160019</td>\n",
       "      <td>10.727593</td>\n",
       "      <td>10.919412</td>\n",
       "      <td>11.153538</td>\n",
       "      <td>10.902894</td>\n",
       "      <td>10.707506</td>\n",
       "      <td>10.714524</td>\n",
       "      <td>10.784154</td>\n",
       "      <td>10.915497</td>\n",
       "      <td>11.079883</td>\n",
       "      <td>11.276442</td>\n",
       "      <td>11.504334</td>\n",
       "      <td>11.790535</td>\n",
       "      <td>12.162067</td>\n",
       "      <td>12.562678</td>\n",
       "      <td>12.963962</td>\n",
       "      <td>13.282029</td>\n",
       "      <td>13.405298</td>\n",
       "      <td>12.888943</td>\n",
       "      <td>11.371571</td>\n",
       "      <td>8.714115</td>\n",
       "      <td>4.860948</td>\n",
       "      <td>-0.243444</td>\n",
       "      <td>-6.487791</td>\n",
       "      <td>-13.696223</td>\n",
       "      <td>-22.591176</td>\n",
       "      <td>-33.367569</td>\n",
       "      <td>-43.377934</td>\n",
       "      <td>-52.989372</td>\n",
       "      <td>-62.541103</td>\n",
       "      <td>-72.400035</td>\n",
       "      <td>-82.738539</td>\n",
       "      <td>-93.701126</td>\n",
       "      <td>-105.209999</td>\n",
       "      <td>-116.992841</td>\n",
       "      <td>-128.777253</td>\n",
       "      <td>-140.290755</td>\n",
       "      <td>39.876474</td>\n",
       "      <td>-58.153225</td>\n",
       "      <td>-233.394404</td>\n",
       "      <td>59.456172</td>\n",
       "      <td>647.143129</td>\n",
       "      <td>900.689655</td>\n",
       "      <td>900.344828</td>\n",
       "      <td>182.111111</td>\n",
       "      <td>119.444444</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>39.966667</td>\n",
       "      <td>68.782483</td>\n",
       "      <td>40.210325</td>\n",
       "      <td>-60.883929</td>\n",
       "      <td>-239.928115</td>\n",
       "      <td>53.003391</td>\n",
       "      <td>645.257053</td>\n",
       "      <td>906.666667</td>\n",
       "      <td>903.333333</td>\n",
       "      <td>183.333333</td>\n",
       "      <td>108.333333</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>40.0</td>\n",
       "      <td>70.588235</td>\n",
       "      <td>25.344543</td>\n",
       "      <td>17.332724</td>\n",
       "      <td>32.532097</td>\n",
       "      <td>35.313818</td>\n",
       "      <td>30.164765</td>\n",
       "      <td>35.291662</td>\n",
       "      <td>95.457538</td>\n",
       "      <td>4.673276</td>\n",
       "      <td>42.138074</td>\n",
       "      <td>75.750076</td>\n",
       "      <td>1.277585</td>\n",
       "      <td>2.604269</td>\n",
       "      <td>109.443646</td>\n",
       "      <td>4.679702</td>\n",
       "      <td>-74.585329</td>\n",
       "      <td>229.882472</td>\n",
       "      <td>702.155116</td>\n",
       "      <td>993.333333</td>\n",
       "      <td>1206.666667</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>353.333333</td>\n",
       "      <td>43.0</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>-49.105674</td>\n",
       "      <td>-96.903808</td>\n",
       "      <td>-264.125043</td>\n",
       "      <td>24.672119</td>\n",
       "      <td>600.110386</td>\n",
       "      <td>840.000000</td>\n",
       "      <td>693.333333</td>\n",
       "      <td>166.666667</td>\n",
       "      <td>76.666667</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>36.0</td>\n",
       "      <td>60.402685</td>\n",
       "      <td>900.689655</td>\n",
       "      <td>35.916341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.532400</td>\n",
       "      <td>26.862158</td>\n",
       "      <td>0.039876</td>\n",
       "      <td>0.029458</td>\n",
       "      <td>906.666667</td>\n",
       "      <td>29.652</td>\n",
       "      <td>0.032704</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>51.724138</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>78.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056782</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.988822</td>\n",
       "      <td>-2.868542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93.934773</td>\n",
       "      <td>96.321661</td>\n",
       "      <td>98.295196</td>\n",
       "      <td>99.880049</td>\n",
       "      <td>101.083010</td>\n",
       "      <td>101.886574</td>\n",
       "      <td>102.264761</td>\n",
       "      <td>102.163024</td>\n",
       "      <td>101.506803</td>\n",
       "      <td>100.195110</td>\n",
       "      <td>98.126172</td>\n",
       "      <td>95.222210</td>\n",
       "      <td>91.431148</td>\n",
       "      <td>86.752678</td>\n",
       "      <td>81.228851</td>\n",
       "      <td>74.940227</td>\n",
       "      <td>68.000313</td>\n",
       "      <td>60.538035</td>\n",
       "      <td>52.681489</td>\n",
       "      <td>44.537425</td>\n",
       "      <td>36.211743</td>\n",
       "      <td>27.810769</td>\n",
       "      <td>19.446370</td>\n",
       "      <td>11.247902</td>\n",
       "      <td>3.348954</td>\n",
       "      <td>-4.113960</td>\n",
       "      <td>-11.030897</td>\n",
       "      <td>-17.332613</td>\n",
       "      <td>-22.992282</td>\n",
       "      <td>-28.037056</td>\n",
       "      <td>-32.530130</td>\n",
       "      <td>-36.557932</td>\n",
       "      <td>-40.216883</td>\n",
       "      <td>-43.578368</td>\n",
       "      <td>-46.703690</td>\n",
       "      <td>-49.628263</td>\n",
       "      <td>-52.357337</td>\n",
       "      <td>-54.874966</td>\n",
       "      <td>-57.155542</td>\n",
       "      <td>-59.199252</td>\n",
       "      <td>-61.009848</td>\n",
       "      <td>-62.630953</td>\n",
       "      <td>-64.150748</td>\n",
       "      <td>-65.699817</td>\n",
       "      <td>-67.385742</td>\n",
       "      <td>-69.230685</td>\n",
       "      <td>-71.141027</td>\n",
       "      <td>-72.708620</td>\n",
       "      <td>-72.998795</td>\n",
       "      <td>-70.435373</td>\n",
       "      <td>-62.863880</td>\n",
       "      <td>-47.622877</td>\n",
       "      <td>-21.771275</td>\n",
       "      <td>17.289063</td>\n",
       "      <td>70.894554</td>\n",
       "      <td>138.112680</td>\n",
       "      <td>215.152712</td>\n",
       "      <td>295.042245</td>\n",
       "      <td>367.299027</td>\n",
       "      <td>419.312176</td>\n",
       "      <td>438.598173</td>\n",
       "      <td>415.789745</td>\n",
       "      <td>347.052507</td>\n",
       "      <td>235.846882</td>\n",
       "      <td>93.625049</td>\n",
       "      <td>-62.150324</td>\n",
       "      <td>-211.237791</td>\n",
       "      <td>-335.073622</td>\n",
       "      <td>-420.412059</td>\n",
       "      <td>-462.040344</td>\n",
       "      <td>-462.490321</td>\n",
       "      <td>-430.414125</td>\n",
       "      <td>-378.134641</td>\n",
       "      <td>-318.092101</td>\n",
       "      <td>-260.336402</td>\n",
       "      <td>-210.957017</td>\n",
       "      <td>-172.521019</td>\n",
       "      <td>-144.813667</td>\n",
       "      <td>-125.782409</td>\n",
       "      <td>-112.850971</td>\n",
       "      <td>-103.730874</td>\n",
       "      <td>-96.728690</td>\n",
       "      <td>-90.808842</td>\n",
       "      <td>-85.418809</td>\n",
       "      <td>-80.300647</td>\n",
       "      <td>-75.360636</td>\n",
       "      <td>-70.559441</td>\n",
       "      <td>-65.876624</td>\n",
       "      <td>-61.274741</td>\n",
       "      <td>-56.705314</td>\n",
       "      <td>-52.112233</td>\n",
       "      <td>-47.446709</td>\n",
       "      <td>-42.658284</td>\n",
       "      <td>-37.682861</td>\n",
       "      <td>-32.441836</td>\n",
       "      <td>-26.848074</td>\n",
       "      <td>-20.810173</td>\n",
       "      <td>-14.248699</td>\n",
       "      <td>-7.094895</td>\n",
       "      <td>0.691373</td>\n",
       "      <td>9.116067</td>\n",
       "      <td>18.152261</td>\n",
       "      <td>27.717923</td>\n",
       "      <td>37.707966</td>\n",
       "      <td>48.007073</td>\n",
       "      <td>58.502521</td>\n",
       "      <td>69.082471</td>\n",
       "      <td>79.622299</td>\n",
       "      <td>90.016223</td>\n",
       "      <td>100.202098</td>\n",
       "      <td>110.126379</td>\n",
       "      <td>119.726610</td>\n",
       "      <td>128.929720</td>\n",
       "      <td>137.676824</td>\n",
       "      <td>145.891606</td>\n",
       "      <td>153.435455</td>\n",
       "      <td>160.164313</td>\n",
       "      <td>165.942354</td>\n",
       "      <td>170.644557</td>\n",
       "      <td>174.143031</td>\n",
       "      <td>176.324966</td>\n",
       "      <td>177.087934</td>\n",
       "      <td>176.300569</td>\n",
       "      <td>173.811111</td>\n",
       "      <td>169.493124</td>\n",
       "      <td>163.277119</td>\n",
       "      <td>155.155237</td>\n",
       "      <td>145.209874</td>\n",
       "      <td>133.635465</td>\n",
       "      <td>120.755127</td>\n",
       "      <td>106.931333</td>\n",
       "      <td>92.532554</td>\n",
       "      <td>77.910591</td>\n",
       "      <td>63.393711</td>\n",
       "      <td>49.259707</td>\n",
       "      <td>35.698263</td>\n",
       "      <td>22.857088</td>\n",
       "      <td>10.839758</td>\n",
       "      <td>-0.266097</td>\n",
       "      <td>-10.408895</td>\n",
       "      <td>-19.565847</td>\n",
       "      <td>-27.730581</td>\n",
       "      <td>-34.910577</td>\n",
       "      <td>-41.134871</td>\n",
       "      <td>-46.470723</td>\n",
       "      <td>-50.990708</td>\n",
       "      <td>-54.768018</td>\n",
       "      <td>-57.873890</td>\n",
       "      <td>-60.390427</td>\n",
       "      <td>-62.378956</td>\n",
       "      <td>-63.863361</td>\n",
       "      <td>-64.863401</td>\n",
       "      <td>-65.371189</td>\n",
       "      <td>-65.339645</td>\n",
       "      <td>-64.657263</td>\n",
       "      <td>-63.150228</td>\n",
       "      <td>-60.590096</td>\n",
       "      <td>-56.677100</td>\n",
       "      <td>-51.050391</td>\n",
       "      <td>-43.317927</td>\n",
       "      <td>-33.130806</td>\n",
       "      <td>-20.244780</td>\n",
       "      <td>-4.586891</td>\n",
       "      <td>13.648401</td>\n",
       "      <td>33.897967</td>\n",
       "      <td>55.150548</td>\n",
       "      <td>75.930970</td>\n",
       "      <td>94.344611</td>\n",
       "      <td>108.240239</td>\n",
       "      <td>115.515585</td>\n",
       "      <td>114.497684</td>\n",
       "      <td>104.338607</td>\n",
       "      <td>85.284686</td>\n",
       "      <td>58.703415</td>\n",
       "      <td>26.855660</td>\n",
       "      <td>-7.498386</td>\n",
       "      <td>-41.627010</td>\n",
       "      <td>-73.297095</td>\n",
       "      <td>-100.998084</td>\n",
       "      <td>-123.892863</td>\n",
       "      <td>107.629654</td>\n",
       "      <td>112.723087</td>\n",
       "      <td>111.981647</td>\n",
       "      <td>114.826471</td>\n",
       "      <td>114.870064</td>\n",
       "      <td>115.673503</td>\n",
       "      <td>123.098430</td>\n",
       "      <td>125.574594</td>\n",
       "      <td>130.150533</td>\n",
       "      <td>135.163778</td>\n",
       "      <td>126.935168</td>\n",
       "      <td>125.865308</td>\n",
       "      <td>119.558893</td>\n",
       "      <td>114.046664</td>\n",
       "      <td>110.947792</td>\n",
       "      <td>98.044770</td>\n",
       "      <td>91.379573</td>\n",
       "      <td>77.164003</td>\n",
       "      <td>63.735822</td>\n",
       "      <td>47.746858</td>\n",
       "      <td>29.655875</td>\n",
       "      <td>14.209798</td>\n",
       "      <td>11.856464</td>\n",
       "      <td>4.680618</td>\n",
       "      <td>-6.835062</td>\n",
       "      <td>-12.102879</td>\n",
       "      <td>-20.113824</td>\n",
       "      <td>-27.103506</td>\n",
       "      <td>-29.549029</td>\n",
       "      <td>-35.383631</td>\n",
       "      <td>-39.299388</td>\n",
       "      <td>-40.132215</td>\n",
       "      <td>-42.498380</td>\n",
       "      <td>-46.566131</td>\n",
       "      <td>-48.889620</td>\n",
       "      <td>-50.790997</td>\n",
       "      <td>-53.888348</td>\n",
       "      <td>-55.314287</td>\n",
       "      <td>-54.988403</td>\n",
       "      <td>-61.489536</td>\n",
       "      <td>-62.389415</td>\n",
       "      <td>-64.478688</td>\n",
       "      <td>-65.504133</td>\n",
       "      <td>-66.332714</td>\n",
       "      <td>-65.690576</td>\n",
       "      <td>-67.785271</td>\n",
       "      <td>-69.406010</td>\n",
       "      <td>-71.687211</td>\n",
       "      <td>-73.312781</td>\n",
       "      <td>-69.983849</td>\n",
       "      <td>-61.497786</td>\n",
       "      <td>-44.610199</td>\n",
       "      <td>-18.840683</td>\n",
       "      <td>18.212576</td>\n",
       "      <td>72.038395</td>\n",
       "      <td>140.175401</td>\n",
       "      <td>217.213119</td>\n",
       "      <td>297.894296</td>\n",
       "      <td>367.117683</td>\n",
       "      <td>417.449202</td>\n",
       "      <td>437.149473</td>\n",
       "      <td>407.640138</td>\n",
       "      <td>341.579922</td>\n",
       "      <td>230.636125</td>\n",
       "      <td>84.757130</td>\n",
       "      <td>-69.091241</td>\n",
       "      <td>-213.653265</td>\n",
       "      <td>-337.223887</td>\n",
       "      <td>-420.590955</td>\n",
       "      <td>-460.802000</td>\n",
       "      <td>...</td>\n",
       "      <td>-163.775586</td>\n",
       "      <td>-168.771061</td>\n",
       "      <td>-172.394643</td>\n",
       "      <td>-174.925776</td>\n",
       "      <td>-176.560834</td>\n",
       "      <td>-177.385332</td>\n",
       "      <td>-177.346131</td>\n",
       "      <td>-176.445863</td>\n",
       "      <td>-174.715135</td>\n",
       "      <td>-172.240285</td>\n",
       "      <td>-169.135580</td>\n",
       "      <td>-165.487644</td>\n",
       "      <td>-161.438765</td>\n",
       "      <td>-157.131323</td>\n",
       "      <td>-152.624433</td>\n",
       "      <td>-152.350999</td>\n",
       "      <td>-153.520596</td>\n",
       "      <td>-153.995098</td>\n",
       "      <td>-153.693602</td>\n",
       "      <td>-152.479622</td>\n",
       "      <td>-150.022180</td>\n",
       "      <td>-146.262892</td>\n",
       "      <td>-162.282513</td>\n",
       "      <td>-180.724026</td>\n",
       "      <td>-199.408129</td>\n",
       "      <td>-218.417858</td>\n",
       "      <td>-237.280836</td>\n",
       "      <td>-255.358154</td>\n",
       "      <td>-271.399919</td>\n",
       "      <td>-284.211912</td>\n",
       "      <td>-291.683349</td>\n",
       "      <td>-291.286863</td>\n",
       "      <td>-280.356260</td>\n",
       "      <td>-255.975375</td>\n",
       "      <td>-215.950270</td>\n",
       "      <td>-158.892523</td>\n",
       "      <td>-85.691418</td>\n",
       "      <td>-0.013901</td>\n",
       "      <td>91.056566</td>\n",
       "      <td>177.186706</td>\n",
       "      <td>245.821227</td>\n",
       "      <td>283.905058</td>\n",
       "      <td>280.577788</td>\n",
       "      <td>230.118094</td>\n",
       "      <td>115.698626</td>\n",
       "      <td>-29.526101</td>\n",
       "      <td>-178.822886</td>\n",
       "      <td>-312.327657</td>\n",
       "      <td>-444.057526</td>\n",
       "      <td>-543.993562</td>\n",
       "      <td>-598.565706</td>\n",
       "      <td>-609.729110</td>\n",
       "      <td>-606.674266</td>\n",
       "      <td>-629.600410</td>\n",
       "      <td>-633.769885</td>\n",
       "      <td>-626.041381</td>\n",
       "      <td>-611.274067</td>\n",
       "      <td>-591.772063</td>\n",
       "      <td>-566.701128</td>\n",
       "      <td>-535.033109</td>\n",
       "      <td>-495.712601</td>\n",
       "      <td>-449.018041</td>\n",
       "      <td>-396.228349</td>\n",
       "      <td>-339.706229</td>\n",
       "      <td>-282.787013</td>\n",
       "      <td>-229.000846</td>\n",
       "      <td>-181.155968</td>\n",
       "      <td>-167.427340</td>\n",
       "      <td>-162.233406</td>\n",
       "      <td>-156.945550</td>\n",
       "      <td>-151.677099</td>\n",
       "      <td>-146.541540</td>\n",
       "      <td>-141.652490</td>\n",
       "      <td>-137.040335</td>\n",
       "      <td>-132.596639</td>\n",
       "      <td>-128.101888</td>\n",
       "      <td>-123.308791</td>\n",
       "      <td>-117.942240</td>\n",
       "      <td>-111.643717</td>\n",
       "      <td>-104.137927</td>\n",
       "      <td>-95.371647</td>\n",
       "      <td>-85.513697</td>\n",
       "      <td>-74.788242</td>\n",
       "      <td>-63.391446</td>\n",
       "      <td>-51.658117</td>\n",
       "      <td>-39.950605</td>\n",
       "      <td>-28.519909</td>\n",
       "      <td>-17.450140</td>\n",
       "      <td>-6.741875</td>\n",
       "      <td>3.548940</td>\n",
       "      <td>9.319677</td>\n",
       "      <td>15.056802</td>\n",
       "      <td>22.137801</td>\n",
       "      <td>30.811861</td>\n",
       "      <td>41.217028</td>\n",
       "      <td>53.130239</td>\n",
       "      <td>66.217340</td>\n",
       "      <td>80.088677</td>\n",
       "      <td>74.736841</td>\n",
       "      <td>65.604044</td>\n",
       "      <td>56.567211</td>\n",
       "      <td>47.433238</td>\n",
       "      <td>37.953482</td>\n",
       "      <td>27.962645</td>\n",
       "      <td>17.128758</td>\n",
       "      <td>5.286510</td>\n",
       "      <td>-7.534992</td>\n",
       "      <td>-21.140009</td>\n",
       "      <td>-36.671868</td>\n",
       "      <td>-53.222370</td>\n",
       "      <td>-69.121627</td>\n",
       "      <td>-84.507556</td>\n",
       "      <td>-99.351622</td>\n",
       "      <td>-113.625523</td>\n",
       "      <td>-126.967871</td>\n",
       "      <td>-138.878652</td>\n",
       "      <td>-148.997014</td>\n",
       "      <td>-157.240162</td>\n",
       "      <td>-163.775586</td>\n",
       "      <td>-168.771061</td>\n",
       "      <td>-172.394643</td>\n",
       "      <td>-174.925776</td>\n",
       "      <td>-176.560834</td>\n",
       "      <td>-177.385332</td>\n",
       "      <td>-177.346131</td>\n",
       "      <td>-176.445863</td>\n",
       "      <td>-174.715135</td>\n",
       "      <td>-172.240285</td>\n",
       "      <td>-169.135580</td>\n",
       "      <td>-165.487644</td>\n",
       "      <td>-161.438765</td>\n",
       "      <td>-157.131323</td>\n",
       "      <td>-152.624433</td>\n",
       "      <td>-147.977261</td>\n",
       "      <td>-145.401889</td>\n",
       "      <td>-145.015917</td>\n",
       "      <td>-144.483418</td>\n",
       "      <td>-143.668490</td>\n",
       "      <td>-144.000700</td>\n",
       "      <td>-162.282513</td>\n",
       "      <td>-180.724026</td>\n",
       "      <td>-199.408129</td>\n",
       "      <td>-218.417858</td>\n",
       "      <td>-237.280836</td>\n",
       "      <td>-255.358154</td>\n",
       "      <td>-271.399919</td>\n",
       "      <td>-284.211912</td>\n",
       "      <td>-291.683349</td>\n",
       "      <td>-291.286863</td>\n",
       "      <td>-311.933819</td>\n",
       "      <td>-412.791517</td>\n",
       "      <td>-476.535978</td>\n",
       "      <td>-557.466529</td>\n",
       "      <td>-606.674266</td>\n",
       "      <td>-629.600410</td>\n",
       "      <td>-633.769885</td>\n",
       "      <td>-626.041381</td>\n",
       "      <td>-611.274067</td>\n",
       "      <td>-591.772063</td>\n",
       "      <td>-566.701128</td>\n",
       "      <td>-587.411075</td>\n",
       "      <td>146.331552</td>\n",
       "      <td>-80.614658</td>\n",
       "      <td>-452.181306</td>\n",
       "      <td>79.535368</td>\n",
       "      <td>439.802266</td>\n",
       "      <td>406.615385</td>\n",
       "      <td>2004.871795</td>\n",
       "      <td>171.111111</td>\n",
       "      <td>131.666667</td>\n",
       "      <td>96.190476</td>\n",
       "      <td>33.785714</td>\n",
       "      <td>158.285005</td>\n",
       "      <td>184.039253</td>\n",
       "      <td>-77.021869</td>\n",
       "      <td>-448.663312</td>\n",
       "      <td>-36.750449</td>\n",
       "      <td>437.976693</td>\n",
       "      <td>383.333333</td>\n",
       "      <td>1656.666667</td>\n",
       "      <td>186.666667</td>\n",
       "      <td>146.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>166.666667</td>\n",
       "      <td>111.562434</td>\n",
       "      <td>44.520546</td>\n",
       "      <td>54.550278</td>\n",
       "      <td>214.536985</td>\n",
       "      <td>51.869951</td>\n",
       "      <td>70.178021</td>\n",
       "      <td>973.407633</td>\n",
       "      <td>24.393887</td>\n",
       "      <td>44.716923</td>\n",
       "      <td>26.721882</td>\n",
       "      <td>9.274994</td>\n",
       "      <td>15.935667</td>\n",
       "      <td>340.851880</td>\n",
       "      <td>26.990479</td>\n",
       "      <td>-330.096571</td>\n",
       "      <td>488.233931</td>\n",
       "      <td>582.207504</td>\n",
       "      <td>676.666667</td>\n",
       "      <td>4160.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>183.333333</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>178.217822</td>\n",
       "      <td>-56.712875</td>\n",
       "      <td>-291.286863</td>\n",
       "      <td>-626.041381</td>\n",
       "      <td>-113.862059</td>\n",
       "      <td>283.905058</td>\n",
       "      <td>336.666667</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>136.666667</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>16.0</td>\n",
       "      <td>88.669951</td>\n",
       "      <td>406.615385</td>\n",
       "      <td>70.724162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.910973</td>\n",
       "      <td>94.652650</td>\n",
       "      <td>0.173934</td>\n",
       "      <td>0.230958</td>\n",
       "      <td>383.333333</td>\n",
       "      <td>34.594</td>\n",
       "      <td>0.090245</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>32.307692</td>\n",
       "      <td>63.076923</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>31.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120121</td>\n",
       "      <td>0.058922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.670907</td>\n",
       "      <td>-2.119254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-26.443108</td>\n",
       "      <td>-25.117556</td>\n",
       "      <td>-23.562392</td>\n",
       "      <td>-21.706623</td>\n",
       "      <td>-19.477371</td>\n",
       "      <td>-16.804914</td>\n",
       "      <td>-13.620787</td>\n",
       "      <td>-9.884917</td>\n",
       "      <td>-5.556579</td>\n",
       "      <td>-0.601962</td>\n",
       "      <td>4.998901</td>\n",
       "      <td>11.223096</td>\n",
       "      <td>17.979587</td>\n",
       "      <td>25.132589</td>\n",
       "      <td>32.476317</td>\n",
       "      <td>39.735005</td>\n",
       "      <td>46.600785</td>\n",
       "      <td>52.767789</td>\n",
       "      <td>57.988977</td>\n",
       "      <td>62.035734</td>\n",
       "      <td>64.698511</td>\n",
       "      <td>65.846801</td>\n",
       "      <td>65.370433</td>\n",
       "      <td>63.226290</td>\n",
       "      <td>59.454722</td>\n",
       "      <td>54.206696</td>\n",
       "      <td>47.740005</td>\n",
       "      <td>40.360550</td>\n",
       "      <td>32.452011</td>\n",
       "      <td>24.387448</td>\n",
       "      <td>16.484478</td>\n",
       "      <td>9.002098</td>\n",
       "      <td>2.135630</td>\n",
       "      <td>-3.994656</td>\n",
       "      <td>-9.334721</td>\n",
       "      <td>-13.882242</td>\n",
       "      <td>-17.703673</td>\n",
       "      <td>-20.919098</td>\n",
       "      <td>-23.668153</td>\n",
       "      <td>-26.085404</td>\n",
       "      <td>-28.309194</td>\n",
       "      <td>-30.458284</td>\n",
       "      <td>-32.642585</td>\n",
       "      <td>-34.982099</td>\n",
       "      <td>-37.632168</td>\n",
       "      <td>-40.726018</td>\n",
       "      <td>-44.356453</td>\n",
       "      <td>-48.531655</td>\n",
       "      <td>-53.083636</td>\n",
       "      <td>-57.529977</td>\n",
       "      <td>-60.878109</td>\n",
       "      <td>-61.560911</td>\n",
       "      <td>-57.473317</td>\n",
       "      <td>-46.237448</td>\n",
       "      <td>-25.671039</td>\n",
       "      <td>5.252352</td>\n",
       "      <td>45.362956</td>\n",
       "      <td>90.913493</td>\n",
       "      <td>134.843700</td>\n",
       "      <td>167.760143</td>\n",
       "      <td>179.797950</td>\n",
       "      <td>163.502121</td>\n",
       "      <td>116.572458</td>\n",
       "      <td>42.160252</td>\n",
       "      <td>-50.097031</td>\n",
       "      <td>-146.043595</td>\n",
       "      <td>-230.623415</td>\n",
       "      <td>-291.688366</td>\n",
       "      <td>-323.108740</td>\n",
       "      <td>-324.666591</td>\n",
       "      <td>-301.599948</td>\n",
       "      <td>-262.806744</td>\n",
       "      <td>-217.647223</td>\n",
       "      <td>-173.732447</td>\n",
       "      <td>-135.645243</td>\n",
       "      <td>-105.166837</td>\n",
       "      <td>-82.234537</td>\n",
       "      <td>-65.518108</td>\n",
       "      <td>-53.415333</td>\n",
       "      <td>-44.563992</td>\n",
       "      <td>-37.976322</td>\n",
       "      <td>-33.013113</td>\n",
       "      <td>-29.162108</td>\n",
       "      <td>-26.038627</td>\n",
       "      <td>-23.444265</td>\n",
       "      <td>-21.270299</td>\n",
       "      <td>-19.442125</td>\n",
       "      <td>-17.864959</td>\n",
       "      <td>-16.430783</td>\n",
       "      <td>-15.040430</td>\n",
       "      <td>-13.565713</td>\n",
       "      <td>-11.912549</td>\n",
       "      <td>-10.003286</td>\n",
       "      <td>-7.776071</td>\n",
       "      <td>-5.174756</td>\n",
       "      <td>-2.148263</td>\n",
       "      <td>1.321626</td>\n",
       "      <td>5.244891</td>\n",
       "      <td>9.628951</td>\n",
       "      <td>14.486864</td>\n",
       "      <td>19.828494</td>\n",
       "      <td>25.659242</td>\n",
       "      <td>32.008460</td>\n",
       "      <td>38.933874</td>\n",
       "      <td>46.499495</td>\n",
       "      <td>54.730806</td>\n",
       "      <td>63.622977</td>\n",
       "      <td>73.150344</td>\n",
       "      <td>83.239911</td>\n",
       "      <td>93.723381</td>\n",
       "      <td>104.344112</td>\n",
       "      <td>114.831622</td>\n",
       "      <td>124.899081</td>\n",
       "      <td>134.271095</td>\n",
       "      <td>142.691929</td>\n",
       "      <td>150.000635</td>\n",
       "      <td>156.147482</td>\n",
       "      <td>161.142192</td>\n",
       "      <td>165.045739</td>\n",
       "      <td>167.909117</td>\n",
       "      <td>169.741779</td>\n",
       "      <td>170.473756</td>\n",
       "      <td>169.928512</td>\n",
       "      <td>167.822308</td>\n",
       "      <td>163.834906</td>\n",
       "      <td>157.697947</td>\n",
       "      <td>149.300998</td>\n",
       "      <td>138.687759</td>\n",
       "      <td>126.139382</td>\n",
       "      <td>112.174458</td>\n",
       "      <td>97.412638</td>\n",
       "      <td>82.457823</td>\n",
       "      <td>67.770625</td>\n",
       "      <td>53.704330</td>\n",
       "      <td>40.431653</td>\n",
       "      <td>27.969335</td>\n",
       "      <td>16.269039</td>\n",
       "      <td>5.312657</td>\n",
       "      <td>-4.839718</td>\n",
       "      <td>-14.094009</td>\n",
       "      <td>-22.370136</td>\n",
       "      <td>-29.599505</td>\n",
       "      <td>-35.735118</td>\n",
       "      <td>-40.795135</td>\n",
       "      <td>-44.862880</td>\n",
       "      <td>-48.074215</td>\n",
       "      <td>-50.537369</td>\n",
       "      <td>-52.357552</td>\n",
       "      <td>-53.635698</td>\n",
       "      <td>-54.462781</td>\n",
       "      <td>-54.901504</td>\n",
       "      <td>-54.986930</td>\n",
       "      <td>-54.762462</td>\n",
       "      <td>-54.267851</td>\n",
       "      <td>-53.542340</td>\n",
       "      <td>-52.616460</td>\n",
       "      <td>-51.536013</td>\n",
       "      <td>-50.355754</td>\n",
       "      <td>-49.116657</td>\n",
       "      <td>-47.850331</td>\n",
       "      <td>-46.579010</td>\n",
       "      <td>-45.325021</td>\n",
       "      <td>-44.096886</td>\n",
       "      <td>-42.889948</td>\n",
       "      <td>-41.698991</td>\n",
       "      <td>-40.516969</td>\n",
       "      <td>-39.335632</td>\n",
       "      <td>-38.145515</td>\n",
       "      <td>-36.934041</td>\n",
       "      <td>-35.698137</td>\n",
       "      <td>-34.439807</td>\n",
       "      <td>-33.157289</td>\n",
       "      <td>-31.844416</td>\n",
       "      <td>-30.490612</td>\n",
       "      <td>-29.089728</td>\n",
       "      <td>-27.616675</td>\n",
       "      <td>-26.033107</td>\n",
       "      <td>-24.306985</td>\n",
       "      <td>-22.413841</td>\n",
       "      <td>-20.341192</td>\n",
       "      <td>-31.555943</td>\n",
       "      <td>-31.032560</td>\n",
       "      <td>-29.101405</td>\n",
       "      <td>-26.660622</td>\n",
       "      <td>-25.314319</td>\n",
       "      <td>-23.417416</td>\n",
       "      <td>-20.711332</td>\n",
       "      <td>-16.520459</td>\n",
       "      <td>-11.350780</td>\n",
       "      <td>-6.365956</td>\n",
       "      <td>-0.551104</td>\n",
       "      <td>4.315769</td>\n",
       "      <td>12.254081</td>\n",
       "      <td>20.626238</td>\n",
       "      <td>28.361368</td>\n",
       "      <td>36.049931</td>\n",
       "      <td>42.387701</td>\n",
       "      <td>47.419520</td>\n",
       "      <td>52.584072</td>\n",
       "      <td>56.572827</td>\n",
       "      <td>59.192540</td>\n",
       "      <td>62.752809</td>\n",
       "      <td>65.322379</td>\n",
       "      <td>60.827687</td>\n",
       "      <td>58.392142</td>\n",
       "      <td>51.879037</td>\n",
       "      <td>47.053124</td>\n",
       "      <td>40.166896</td>\n",
       "      <td>30.871447</td>\n",
       "      <td>23.597805</td>\n",
       "      <td>17.116210</td>\n",
       "      <td>10.282344</td>\n",
       "      <td>3.459494</td>\n",
       "      <td>-3.701511</td>\n",
       "      <td>-8.061931</td>\n",
       "      <td>-13.602216</td>\n",
       "      <td>-17.441389</td>\n",
       "      <td>-21.265467</td>\n",
       "      <td>-24.714773</td>\n",
       "      <td>-27.966250</td>\n",
       "      <td>-31.582020</td>\n",
       "      <td>-33.212187</td>\n",
       "      <td>-34.796037</td>\n",
       "      <td>-37.213718</td>\n",
       "      <td>-40.019164</td>\n",
       "      <td>-44.837830</td>\n",
       "      <td>-48.957177</td>\n",
       "      <td>-54.180722</td>\n",
       "      <td>-59.943377</td>\n",
       "      <td>-64.427049</td>\n",
       "      <td>-67.846767</td>\n",
       "      <td>-69.773475</td>\n",
       "      <td>-67.736436</td>\n",
       "      <td>-57.363968</td>\n",
       "      <td>-40.858358</td>\n",
       "      <td>-12.671629</td>\n",
       "      <td>28.083464</td>\n",
       "      <td>71.352293</td>\n",
       "      <td>116.508137</td>\n",
       "      <td>153.190137</td>\n",
       "      <td>169.763444</td>\n",
       "      <td>151.855647</td>\n",
       "      <td>108.543867</td>\n",
       "      <td>39.917412</td>\n",
       "      <td>-55.524664</td>\n",
       "      <td>-157.083108</td>\n",
       "      <td>-239.673435</td>\n",
       "      <td>-305.298202</td>\n",
       "      <td>-338.179450</td>\n",
       "      <td>-333.613276</td>\n",
       "      <td>...</td>\n",
       "      <td>-66.723384</td>\n",
       "      <td>-68.065778</td>\n",
       "      <td>-69.814589</td>\n",
       "      <td>-71.802241</td>\n",
       "      <td>-73.916716</td>\n",
       "      <td>-76.018236</td>\n",
       "      <td>-78.105941</td>\n",
       "      <td>-80.151241</td>\n",
       "      <td>-82.042274</td>\n",
       "      <td>-83.722811</td>\n",
       "      <td>-85.136721</td>\n",
       "      <td>-86.255764</td>\n",
       "      <td>-86.996273</td>\n",
       "      <td>-88.756240</td>\n",
       "      <td>-94.815196</td>\n",
       "      <td>-98.828187</td>\n",
       "      <td>-101.211730</td>\n",
       "      <td>-102.410073</td>\n",
       "      <td>-102.922988</td>\n",
       "      <td>-103.222461</td>\n",
       "      <td>-103.419378</td>\n",
       "      <td>-103.569087</td>\n",
       "      <td>-103.671405</td>\n",
       "      <td>-103.670621</td>\n",
       "      <td>-103.455485</td>\n",
       "      <td>-102.859205</td>\n",
       "      <td>-101.826101</td>\n",
       "      <td>-100.272705</td>\n",
       "      <td>-103.268685</td>\n",
       "      <td>-108.151417</td>\n",
       "      <td>-112.027124</td>\n",
       "      <td>-115.006475</td>\n",
       "      <td>-119.000189</td>\n",
       "      <td>-117.947333</td>\n",
       "      <td>-109.992611</td>\n",
       "      <td>-111.180313</td>\n",
       "      <td>-109.479520</td>\n",
       "      <td>-103.079964</td>\n",
       "      <td>-92.174006</td>\n",
       "      <td>-78.565046</td>\n",
       "      <td>-65.306411</td>\n",
       "      <td>-56.868020</td>\n",
       "      <td>-57.275274</td>\n",
       "      <td>-70.025732</td>\n",
       "      <td>-97.672458</td>\n",
       "      <td>-143.871003</td>\n",
       "      <td>-237.889201</td>\n",
       "      <td>-327.044970</td>\n",
       "      <td>-396.488424</td>\n",
       "      <td>-436.288614</td>\n",
       "      <td>-445.749324</td>\n",
       "      <td>-428.424438</td>\n",
       "      <td>-392.701295</td>\n",
       "      <td>-347.828478</td>\n",
       "      <td>-301.888040</td>\n",
       "      <td>-260.212168</td>\n",
       "      <td>-224.938728</td>\n",
       "      <td>-197.316809</td>\n",
       "      <td>-176.178931</td>\n",
       "      <td>-159.774366</td>\n",
       "      <td>-146.408009</td>\n",
       "      <td>-134.912591</td>\n",
       "      <td>-132.774873</td>\n",
       "      <td>-130.625636</td>\n",
       "      <td>-127.741452</td>\n",
       "      <td>-124.625525</td>\n",
       "      <td>-121.892214</td>\n",
       "      <td>-119.905920</td>\n",
       "      <td>-118.975521</td>\n",
       "      <td>-119.409923</td>\n",
       "      <td>-121.406949</td>\n",
       "      <td>-124.914444</td>\n",
       "      <td>-129.685834</td>\n",
       "      <td>-135.446790</td>\n",
       "      <td>-141.645229</td>\n",
       "      <td>-147.562428</td>\n",
       "      <td>-152.674136</td>\n",
       "      <td>-156.678353</td>\n",
       "      <td>-159.245330</td>\n",
       "      <td>-160.156457</td>\n",
       "      <td>-159.470933</td>\n",
       "      <td>-157.359091</td>\n",
       "      <td>-153.907951</td>\n",
       "      <td>-149.148996</td>\n",
       "      <td>-143.224828</td>\n",
       "      <td>-136.250270</td>\n",
       "      <td>-128.229027</td>\n",
       "      <td>-128.250634</td>\n",
       "      <td>-128.076160</td>\n",
       "      <td>-127.551444</td>\n",
       "      <td>-126.650546</td>\n",
       "      <td>-125.375316</td>\n",
       "      <td>-123.727603</td>\n",
       "      <td>-121.681465</td>\n",
       "      <td>-119.266476</td>\n",
       "      <td>-116.039941</td>\n",
       "      <td>-110.920200</td>\n",
       "      <td>-102.464393</td>\n",
       "      <td>-88.729544</td>\n",
       "      <td>-67.383656</td>\n",
       "      <td>-36.261235</td>\n",
       "      <td>4.497835</td>\n",
       "      <td>52.364989</td>\n",
       "      <td>64.680431</td>\n",
       "      <td>66.821965</td>\n",
       "      <td>67.103166</td>\n",
       "      <td>65.468440</td>\n",
       "      <td>61.862411</td>\n",
       "      <td>56.313237</td>\n",
       "      <td>8.500049</td>\n",
       "      <td>-82.185010</td>\n",
       "      <td>-167.258829</td>\n",
       "      <td>-232.831355</td>\n",
       "      <td>-270.373622</td>\n",
       "      <td>-277.745567</td>\n",
       "      <td>-259.140503</td>\n",
       "      <td>-223.946256</td>\n",
       "      <td>-182.578510</td>\n",
       "      <td>-143.453039</td>\n",
       "      <td>-111.652376</td>\n",
       "      <td>-88.564698</td>\n",
       "      <td>-76.367330</td>\n",
       "      <td>-81.444609</td>\n",
       "      <td>-85.443985</td>\n",
       "      <td>-88.446869</td>\n",
       "      <td>-90.645773</td>\n",
       "      <td>-94.957192</td>\n",
       "      <td>-98.665013</td>\n",
       "      <td>-100.972119</td>\n",
       "      <td>-102.046908</td>\n",
       "      <td>-102.058014</td>\n",
       "      <td>-101.396533</td>\n",
       "      <td>-100.287119</td>\n",
       "      <td>-106.110126</td>\n",
       "      <td>-112.042856</td>\n",
       "      <td>-117.101183</td>\n",
       "      <td>-121.036711</td>\n",
       "      <td>-123.767972</td>\n",
       "      <td>-125.352643</td>\n",
       "      <td>-125.904210</td>\n",
       "      <td>-125.564187</td>\n",
       "      <td>-124.335442</td>\n",
       "      <td>-122.248856</td>\n",
       "      <td>-122.076966</td>\n",
       "      <td>-121.966282</td>\n",
       "      <td>-121.444663</td>\n",
       "      <td>-120.513386</td>\n",
       "      <td>-119.173847</td>\n",
       "      <td>-117.455330</td>\n",
       "      <td>-115.415004</td>\n",
       "      <td>-113.054584</td>\n",
       "      <td>-110.431436</td>\n",
       "      <td>-107.575237</td>\n",
       "      <td>-104.515748</td>\n",
       "      <td>-101.227250</td>\n",
       "      <td>-98.844753</td>\n",
       "      <td>-97.582131</td>\n",
       "      <td>-95.700269</td>\n",
       "      <td>-93.225825</td>\n",
       "      <td>-91.959060</td>\n",
       "      <td>-92.619381</td>\n",
       "      <td>67.107476</td>\n",
       "      <td>-66.970043</td>\n",
       "      <td>-320.795351</td>\n",
       "      <td>176.213541</td>\n",
       "      <td>178.868548</td>\n",
       "      <td>612.424242</td>\n",
       "      <td>626.589147</td>\n",
       "      <td>146.969697</td>\n",
       "      <td>120.378788</td>\n",
       "      <td>98.636364</td>\n",
       "      <td>27.840909</td>\n",
       "      <td>99.131894</td>\n",
       "      <td>66.603497</td>\n",
       "      <td>-71.033316</td>\n",
       "      <td>-327.652237</td>\n",
       "      <td>174.391177</td>\n",
       "      <td>169.697309</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>603.333333</td>\n",
       "      <td>168.333333</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>34.5</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>44.155314</td>\n",
       "      <td>38.968145</td>\n",
       "      <td>47.241720</td>\n",
       "      <td>36.112023</td>\n",
       "      <td>82.853353</td>\n",
       "      <td>69.155671</td>\n",
       "      <td>117.031519</td>\n",
       "      <td>35.354040</td>\n",
       "      <td>31.319597</td>\n",
       "      <td>43.107325</td>\n",
       "      <td>10.293297</td>\n",
       "      <td>6.087403</td>\n",
       "      <td>191.351890</td>\n",
       "      <td>80.127998</td>\n",
       "      <td>-128.076160</td>\n",
       "      <td>240.696324</td>\n",
       "      <td>488.124756</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>1243.333333</td>\n",
       "      <td>216.666667</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>117.647059</td>\n",
       "      <td>-65.650306</td>\n",
       "      <td>-117.947333</td>\n",
       "      <td>-428.424438</td>\n",
       "      <td>66.821965</td>\n",
       "      <td>-56.868020</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>553.333333</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>43.333333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>63.157895</td>\n",
       "      <td>612.424242</td>\n",
       "      <td>69.955185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.534641</td>\n",
       "      <td>84.522923</td>\n",
       "      <td>0.114227</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>22.239</td>\n",
       "      <td>0.037065</td>\n",
       "      <td>30.833333</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>6.285714</td>\n",
       "      <td>109.3750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033902</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.856713</td>\n",
       "      <td>-3.384267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 989 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3           4           5    \\\n",
       "0 -14.498234 -13.234579 -11.628894  -9.684386   -7.431202   -4.918017   \n",
       "1  -1.753309  -0.242555   1.452685   3.257832    5.084793    6.856565   \n",
       "2 -15.740629 -13.864271 -11.691737  -9.261015   -6.574892   -3.650031   \n",
       "3  93.934773  96.321661  98.295196  99.880049  101.083010  101.886574   \n",
       "4 -26.443108 -25.117556 -23.562392 -21.706623  -19.477371  -16.804914   \n",
       "\n",
       "          6           7           8           9          10         11   \\\n",
       "0   -2.186781    0.719700    3.730826    6.763773   9.754217  12.654224   \n",
       "1    8.538980   10.120082   11.629965   13.121732  14.672293  16.334753   \n",
       "2   -0.504923    2.855850    6.435141   10.251569  14.259899  18.406591   \n",
       "3  102.264761  102.163024  101.506803  100.195110  98.126172  95.222210   \n",
       "4  -13.620787   -9.884917   -5.556579   -0.601962   4.998901  11.223096   \n",
       "\n",
       "         12         13         14         15         16         17   \\\n",
       "0  15.387219  17.858503  19.989768  21.740134  23.071220  23.946301   \n",
       "1  18.086824  19.897503  21.708818  23.449324  25.021414  26.338618   \n",
       "2  22.605733  26.734411  30.600296  33.957392  36.579173  38.209511   \n",
       "3  91.431148  86.752678  81.228851  74.940227  68.000313  60.538035   \n",
       "4  17.979587  25.132589  32.476317  39.735005  46.600785  52.767789   \n",
       "\n",
       "         18         19         20         21         22         23   \\\n",
       "0  24.353872  24.313122  23.826370  22.872333  21.427582  19.466121   \n",
       "1  27.360529  28.045984  28.352265  28.204947  27.537580  26.290099   \n",
       "2  38.609889  37.569581  34.983425  30.876804  25.388980  18.786963   \n",
       "3  52.681489  44.537425  36.211743  27.810769  19.446370  11.247902   \n",
       "4  57.988977  62.035734  64.698511  65.846801  65.370433  63.226290   \n",
       "\n",
       "         24         25         26         27         28         29   \\\n",
       "0  16.945076  13.789532   9.954404   5.429056   0.238137  -5.534010   \n",
       "1  24.404062  21.841696  18.586689  14.691006  10.262195   5.426866   \n",
       "2  11.434943   3.763724  -3.796887 -10.933831 -17.445261 -23.219257   \n",
       "3   3.348954  -4.113960 -11.030897 -17.332613 -22.992282 -28.037056   \n",
       "4  59.454722  54.206696  47.740005  40.360550  32.452011  24.387448   \n",
       "\n",
       "         30         31         32         33         34         35   \\\n",
       "0 -11.735787 -18.131505 -24.451479 -30.464007 -35.978735 -40.880341   \n",
       "1   0.328308  -4.883041 -10.036107 -14.982046 -19.622026 -23.880251   \n",
       "2 -28.240323 -32.569955 -36.326282 -39.597041 -42.451623 -44.964229   \n",
       "3 -32.530130 -36.557932 -40.216883 -43.578368 -46.703690 -49.628263   \n",
       "4  16.484478   9.002098   2.135630  -3.994656  -9.334721 -13.882242   \n",
       "\n",
       "         36         37         38         39         40         41   \\\n",
       "0 -45.115909 -48.698300 -51.698581 -54.150485 -56.086609 -57.528309   \n",
       "1 -27.706346 -31.097585 -34.095723 -36.785412 -39.259284 -41.603668   \n",
       "2 -47.192584 -49.179789 -50.943216 -52.488397 -53.810874 -54.886016   \n",
       "3 -52.357337 -54.874966 -57.155542 -59.199252 -61.009848 -62.630953   \n",
       "4 -17.703673 -20.919098 -23.668153 -26.085404 -28.309194 -30.458284   \n",
       "\n",
       "         42         43         44         45         46         47   \\\n",
       "0 -58.489907 -59.003520 -59.099691 -58.844840 -58.264232 -57.272099   \n",
       "1 -43.887478 -46.131266 -48.310392 -50.288354 -51.815995 -52.530699   \n",
       "2 -55.676414 -56.121689 -56.150521 -55.682482 -54.570613 -52.469919   \n",
       "3 -64.150748 -65.699817 -67.385742 -69.230685 -71.141027 -72.708620   \n",
       "4 -32.642585 -34.982099 -37.632168 -40.726018 -44.356453 -48.531655   \n",
       "\n",
       "         48         49         50         51         52         53   \\\n",
       "0 -55.560516 -52.325388 -46.043368 -34.343886 -14.213665  17.301290   \n",
       "1 -51.945275 -49.416204 -44.089653 -35.117344 -21.790658  -3.656500   \n",
       "2 -48.629936 -41.671550 -29.430482  -8.805396  24.099558  73.359048   \n",
       "3 -72.998795 -70.435373 -62.863880 -47.622877 -21.771275  17.289063   \n",
       "4 -53.083636 -57.529977 -60.878109 -61.560911 -57.473317 -46.237448   \n",
       "\n",
       "          54          55          56          57          58          59   \\\n",
       "0   61.813321  118.367141  183.221865  249.319054  307.014239  346.379558   \n",
       "1   19.260505   46.150882   75.344927  104.435743  130.190362  148.560079   \n",
       "2  142.017361  230.231970  333.808784  443.287382  543.791992  617.421342   \n",
       "3   70.894554  138.112680  215.152712  295.042245  367.299027  419.312176   \n",
       "4  -25.671039    5.252352   45.362956   90.913493  134.843700  167.760143   \n",
       "\n",
       "          60          61          62          63          64          65   \\\n",
       "0  360.369182  347.130355  308.973094  252.583987  187.814469  124.789810   \n",
       "1  155.609025  148.581630  126.583574   90.834151   44.890066   -5.244272   \n",
       "2  647.143129  621.881064  540.280601  411.398716  254.311266   93.549033   \n",
       "3  438.598173  415.789745  347.052507  235.846882   93.625049  -62.150324   \n",
       "4  179.797950  163.502121  116.572458   42.160252  -50.097031 -146.043595   \n",
       "\n",
       "          66          67          68          69          70          71   \\\n",
       "0   70.895187   28.894343   -1.227022  -21.555891  -35.419058  -46.058657   \n",
       "1  -52.720740  -91.638008 -118.340765 -131.603079 -132.383179 -123.702039   \n",
       "2  -47.204205 -151.228411 -212.092372 -232.400981 -222.175855 -195.052599   \n",
       "3 -211.237791 -335.073622 -420.412059 -462.040344 -462.490321 -430.414125   \n",
       "4 -230.623415 -291.688366 -323.108740 -324.666591 -301.599948 -262.806744   \n",
       "\n",
       "          72          73          74          75          76          77   \\\n",
       "0  -55.581692  -64.479414  -72.558327  -79.542895  -85.247278  -89.626694   \n",
       "1 -109.549739  -93.408490  -77.708192  -63.882780  -52.613881  -43.970487   \n",
       "2 -163.580845 -135.964085 -115.610621 -102.803016  -95.820323  -92.112165   \n",
       "3 -378.134641 -318.092101 -260.336402 -210.957017 -172.521019 -144.813667   \n",
       "4 -217.647223 -173.732447 -135.645243 -105.166837  -82.234537  -65.518108   \n",
       "\n",
       "          78          79          80         81         82         83   \\\n",
       "0  -92.800152  -95.014692  -96.535539 -97.528265 -98.079411 -98.268462   \n",
       "1  -37.506574  -32.830940  -29.532595 -27.187109 -25.383592 -23.743740   \n",
       "2  -89.606138  -87.341151  -85.039638 -82.624227 -80.074213 -77.401477   \n",
       "3 -125.782409 -112.850971 -103.730874 -96.728690 -90.808842 -85.418809   \n",
       "4  -53.415333  -44.563992  -37.976322 -33.013113 -29.162108 -26.038627   \n",
       "\n",
       "         84         85         86         87         88         89   \\\n",
       "0 -98.182151 -97.877847 -97.381024 -96.717243 -95.922674 -95.021363   \n",
       "1 -22.077385 -20.285667 -18.326909 -16.199944 -13.930626 -11.563092   \n",
       "2 -74.648628 -71.838068 -68.996978 -66.151754 -63.334476 -60.571786   \n",
       "3 -80.300647 -75.360636 -70.559441 -65.876624 -61.274741 -56.705314   \n",
       "4 -23.444265 -21.270299 -19.442125 -17.864959 -16.430783 -15.040430   \n",
       "\n",
       "         90         91         92         93         94         95   \\\n",
       "0 -94.015547 -92.901640 -91.685387 -90.388166 -89.011632 -87.539819   \n",
       "1  -9.099451  -6.510889  -3.826561  -1.134383   1.449127   3.778846   \n",
       "2 -57.870062 -55.242254 -52.685655 -50.179111 -47.686710 -45.184629   \n",
       "3 -52.112233 -47.446709 -42.658284 -37.682861 -32.441836 -26.848074   \n",
       "4 -13.565713 -11.912549 -10.003286  -7.776071  -5.174756  -2.148263   \n",
       "\n",
       "         96         97         98         99         100        101  \\\n",
       "0 -85.974066 -84.309869 -82.534768 -80.618668 -78.546665 -76.313150   \n",
       "1   5.706445   7.089908   7.907028   8.297462   8.465114   8.689247   \n",
       "2 -42.650007 -40.070198 -37.447390 -34.789337 -32.127868 -29.467029   \n",
       "3 -20.810173 -14.248699  -7.094895   0.691373   9.116067  18.152261   \n",
       "4   1.321626   5.244891   9.628951  14.486864  19.828494  25.659242   \n",
       "\n",
       "         102        103        104        105        106        107  \\\n",
       "0 -73.882243 -71.248823 -68.420004 -65.416821 -62.280539 -59.067606   \n",
       "1   9.210981  10.215838  11.809138  13.955674  16.563843  19.503109   \n",
       "2 -26.836778 -24.259648 -21.744252 -19.302879 -16.922780 -14.630050   \n",
       "3  27.717923  37.707966  48.007073  58.502521  69.082471  79.622299   \n",
       "4  32.008460  38.933874  46.499495  54.730806  63.622977  73.150344   \n",
       "\n",
       "         108         109         110         111         112         113  \\\n",
       "0 -55.863958  -52.705899  -49.611246  -46.578907  -43.560687  -40.476442   \n",
       "1  22.692091   26.074763   29.604578   33.285741   37.157341   41.257637   \n",
       "2 -12.416485  -10.267348   -8.177109   -6.138337   -4.167618   -2.254633   \n",
       "3  90.016223  100.202098  110.126379  119.726610  128.929720  137.676824   \n",
       "4  83.239911   93.723381  104.344112  114.831622  124.899081  134.271095   \n",
       "\n",
       "          114         115         116         117         118         119  \\\n",
       "0  -37.218287  -33.699423  -29.858347  -25.650435  -21.043737  -16.026551   \n",
       "1   45.575654   50.092454   54.787490   59.633854   64.585582   69.565754   \n",
       "2   -0.403820    1.372103    3.071582    4.695896    6.257484    7.761428   \n",
       "3  145.891606  153.435455  160.164313  165.942354  170.644557  174.143031   \n",
       "4  142.691929  150.000635  156.147482  161.142192  165.045739  167.909117   \n",
       "\n",
       "          120         121         122         123         124         125  \\\n",
       "0  -10.591853   -4.728458    1.592870    8.417317   15.792974   23.810822   \n",
       "1   74.507773   79.344262   83.976907   88.309008   92.243897   95.723038   \n",
       "2    9.211002   10.603969   11.939981   13.208542   14.415850   15.558875   \n",
       "3  176.324966  177.087934  176.300569  173.811111  169.493124  163.277119   \n",
       "4  169.741779  170.473756  169.928512  167.822308  163.834906  157.697947   \n",
       "\n",
       "          126         127         128         129         130         131  \\\n",
       "0   32.558446   42.093098   52.453073   63.630357   75.536966   87.945192   \n",
       "1   98.698255  101.119834  102.975412  104.231255  104.829082  104.681306   \n",
       "2   16.630901   17.648377   18.622209   19.574417   20.505725   21.431668   \n",
       "3  155.155237  145.209874  133.635465  120.755127  106.931333   92.532554   \n",
       "4  149.300998  138.687759  126.139382  112.174458   97.412638   82.457823   \n",
       "\n",
       "          132         133         134         135         136         137  \\\n",
       "0  100.561262  113.104475  125.296269  136.883797  147.654250  157.488734   \n",
       "1  103.682146  101.734609   98.725887   94.561477   89.215975   82.731479   \n",
       "2   22.377022   23.342474   24.328683   25.341832   26.395472   27.504038   \n",
       "3   77.910591   63.393711   49.259707   35.698263   22.857088   10.839758   \n",
       "4   67.770625   53.704330   40.431653   27.969335   16.269039    5.312657   \n",
       "\n",
       "          138         139         140         141         142         143  \\\n",
       "0  166.321037  174.048829  180.538305  185.606931  189.055862  190.660683   \n",
       "1   75.201715   66.716467   57.402053   47.455438   37.072799   26.420948   \n",
       "2   28.670806   29.918447   31.252918   32.670864   34.166109   35.737994   \n",
       "3   -0.266097  -10.408895  -19.565847  -27.730581  -34.910577  -41.134871   \n",
       "4   -4.839718  -14.094009  -22.370136  -29.599505  -35.735118  -40.795135   \n",
       "\n",
       "          144         145         146         147         148         149  \\\n",
       "0  190.147849  187.280120  181.890234  173.972233  163.655362  151.162823   \n",
       "1   15.652397    5.007733   -5.262165  -14.965980  -23.932277  -32.010308   \n",
       "2   37.391374   39.101451   40.852659   42.609047   44.335579   45.973142   \n",
       "3  -46.470723  -50.990708  -54.768018  -57.873890  -60.390427  -62.378956   \n",
       "4  -44.862880  -48.074215  -50.537369  -52.357552  -53.635698  -54.462781   \n",
       "\n",
       "          150         151         152        153        154        155  \\\n",
       "0  136.866472  121.242203  104.797544  87.898240  70.848631  53.972026   \n",
       "1  -39.052550  -45.039321  -50.000205 -53.988665 -57.112989 -59.480745   \n",
       "2   47.471885   48.801412   49.936898  50.880393  51.610832  52.097922   \n",
       "3  -63.863361  -64.863401  -65.371189 -65.339645 -64.657263 -63.150228   \n",
       "4  -54.901504  -54.986930  -54.762462 -54.267851 -53.542340 -52.616460   \n",
       "\n",
       "         156        157        158        159        160        161  \\\n",
       "0  37.557659  21.829952   7.003641  -6.598808 -18.710091 -29.203930   \n",
       "1 -61.213059 -62.364459 -62.983984 -63.112007 -62.765944 -61.983109   \n",
       "2  52.292901  52.134091  51.542278  50.424426  48.694047  46.277693   \n",
       "3 -60.590096 -56.677100 -51.050391 -43.317927 -33.130806 -20.244780   \n",
       "4 -51.536013 -50.355754 -49.116657 -47.850331 -46.579010 -45.325021   \n",
       "\n",
       "         162        163        164        165        166        167  \\\n",
       "0 -38.047099 -45.283864 -51.052402 -55.594070 -59.139776 -61.876320   \n",
       "1 -60.815151 -59.347888 -57.664004 -55.851767 -53.994705 -52.162079   \n",
       "2  43.131626  39.250156  34.676756  29.482770  23.785931  17.742950   \n",
       "3  -4.586891  13.648401  33.897967  55.150548  75.930970  94.344611   \n",
       "4 -44.096886 -42.889948 -41.698991 -40.516969 -39.335632 -38.145515   \n",
       "\n",
       "          168         169         170         171        172        173  \\\n",
       "0  -63.980914  -65.585412  -66.767905  -67.575868 -68.045530 -68.201873   \n",
       "1  -50.396969  -48.718647  -47.151145  -45.705784 -44.396253 -43.188601   \n",
       "2   11.493966    5.156976   -1.192533   -7.492895 -13.690718 -19.744595   \n",
       "3  108.240239  115.515585  114.497684  104.338607  85.284686  58.703415   \n",
       "4  -36.934041  -35.698137  -34.439807  -33.157289 -31.844416 -30.490612   \n",
       "\n",
       "         174        175        176        177         178         179  \\\n",
       "0 -68.047691 -67.590102 -66.860754 -65.893511  -64.724872  -63.396068   \n",
       "1 -42.066310 -41.028706 -40.076671 -39.219778  -38.435817  -37.742220   \n",
       "2 -25.604752 -31.211202 -36.490062 -41.363742  -45.776887  -49.701018   \n",
       "3  26.855660  -7.498386 -41.627010 -73.297095 -100.998084 -123.892863   \n",
       "4 -29.089728 -27.616675 -26.033107 -24.306985  -22.413841  -20.341192   \n",
       "\n",
       "          180         181         182         183         184         185  \\\n",
       "0  -15.489147  -13.030004  -10.820615   -8.624940   -5.895799   -2.725400   \n",
       "1   -1.219333   -2.887117   -3.042428   -0.180479   -0.035532    1.125449   \n",
       "2  -12.624124  -11.869465  -10.778435   -7.773356   -5.155552   -2.293272   \n",
       "3  107.629654  112.723087  111.981647  114.826471  114.870064  115.673503   \n",
       "4  -31.555943  -31.032560  -29.101405  -26.660622  -25.314319  -23.417416   \n",
       "\n",
       "          186         187         188         189         190         191  \\\n",
       "0   -0.469552    2.266584    6.231460   10.126356   13.440836   15.515698   \n",
       "1    3.783738    7.898919   11.031366   14.846134   16.350363   19.309462   \n",
       "2   -0.425352    3.554757    8.073808   12.018306   16.318128   20.160432   \n",
       "3  123.098430  125.574594  130.150533  135.163778  126.935168  125.865308   \n",
       "4  -20.711332  -16.520459  -11.350780   -6.365956   -0.551104    4.315769   \n",
       "\n",
       "          192         193         194        195        196        197  \\\n",
       "0   18.423118   20.660126   23.277018  24.091516  24.515854  25.127938   \n",
       "1   23.618644   24.622350   23.882625  25.319072  27.576786  29.786530   \n",
       "2   25.424818   29.810309   33.939505  37.288502  39.171516  40.025286   \n",
       "3  119.558893  114.046664  110.947792  98.044770  91.379573  77.164003   \n",
       "4   12.254081   20.626238   28.361368  36.049931  42.387701  47.419520   \n",
       "\n",
       "         198        199        200        201        202        203  \\\n",
       "0  25.930316  27.018542  26.089801  24.913826  24.208905  23.207770   \n",
       "1  28.309271  27.088866  25.951592  28.437887  26.275017  26.474506   \n",
       "2  39.581442  38.437253  35.860205  31.740109  26.133421  19.179903   \n",
       "3  63.735822  47.746858  29.655875  14.209798  11.856464   4.680618   \n",
       "4  52.584072  56.572827  59.192540  62.752809  65.322379  60.827687   \n",
       "\n",
       "         204        205        206        207        208        209  \\\n",
       "0  21.057288  18.180164  15.251634  10.320022   5.526655  -0.329275   \n",
       "1  29.493239  30.233601  26.932193  20.750344  14.169941   7.495982   \n",
       "2  10.977090   1.690469  -7.581131 -13.527236 -18.859527 -24.120911   \n",
       "3  -6.835062 -12.102879 -20.113824 -27.103506 -29.549029 -35.383631   \n",
       "4  58.392142  51.879037  47.053124  40.166896  30.871447  23.597805   \n",
       "\n",
       "         210        211        212        213        214        215  \\\n",
       "0  -6.520551 -14.801481 -20.393518 -27.137130 -32.638144 -36.423865   \n",
       "1   4.068793   1.609599  -2.755134  -8.925125 -14.679182 -21.595700   \n",
       "2 -29.184199 -33.828832 -37.510691 -40.611693 -44.171125 -47.754708   \n",
       "3 -39.299388 -40.132215 -42.498380 -46.566131 -48.889620 -50.790997   \n",
       "4  17.116210  10.282344   3.459494  -3.701511  -8.061931 -13.602216   \n",
       "\n",
       "         216        217        218        219        220        221  \\\n",
       "0 -40.277212 -43.698567 -47.490486 -51.543123 -54.810245 -56.615152   \n",
       "1 -24.854458 -29.247049 -30.952465 -34.040928 -36.345614 -38.808245   \n",
       "2 -50.296313 -52.814126 -54.693051 -56.737631 -58.009052 -58.733156   \n",
       "3 -53.888348 -55.314287 -54.988403 -61.489536 -62.389415 -64.478688   \n",
       "4 -17.441389 -21.265467 -24.714773 -27.966250 -31.582020 -33.212187   \n",
       "\n",
       "         222        223        224        225        226        227  \\\n",
       "0 -58.192914 -58.974315 -57.309944 -58.883707 -59.274490 -60.730837   \n",
       "1 -41.905718 -45.985519 -49.260584 -51.605338 -54.501382 -54.923608   \n",
       "2 -59.667189 -60.116707 -59.545000 -58.676832 -57.325058 -55.091000   \n",
       "3 -65.504133 -66.332714 -65.690576 -67.785271 -69.406010 -71.687211   \n",
       "4 -34.796037 -37.213718 -40.019164 -44.837830 -48.957177 -54.180722   \n",
       "\n",
       "         228        229        230        231        232        233  \\\n",
       "0 -59.857486 -55.610578 -48.135666 -36.108145 -18.188899  10.957213   \n",
       "1 -55.732643 -49.939888 -46.619433 -35.695846 -24.709805 -11.435534   \n",
       "2 -51.230835 -45.776798 -31.882237 -11.905830  22.877433  69.255212   \n",
       "3 -73.312781 -69.983849 -61.497786 -44.610199 -18.840683  18.212576   \n",
       "4 -59.943377 -64.427049 -67.846767 -69.773475 -67.736436 -57.363968   \n",
       "\n",
       "          234         235         236         237         238         239  \\\n",
       "0   55.448484  110.667819  175.120652  235.753106  295.648090  340.506318   \n",
       "1   11.168290   34.970849   67.112959   94.601651  117.972197  136.560382   \n",
       "2  137.888729  225.356285  326.725024  439.109955  542.146524  617.828077   \n",
       "3   72.038395  140.175401  217.213119  297.894296  367.117683  417.449202   \n",
       "4  -40.858358  -12.671629   28.083464   71.352293  116.508137  153.190137   \n",
       "\n",
       "          240         241         242         243         244         245  \\\n",
       "0  356.351168  341.520023  302.930429  245.382878  185.706886  122.376672   \n",
       "1  141.058357  133.278022  115.232093   78.198366   35.353863  -13.861311   \n",
       "2  645.257053  621.640783  542.443148  412.808402  258.607114   99.046169   \n",
       "3  437.149473  407.640138  341.579922  230.636125   84.757130  -69.091241   \n",
       "4  169.763444  151.855647  108.543867   39.917412  -55.524664 -157.083108   \n",
       "\n",
       "          246         247         248         249  ...         739  \\\n",
       "0   67.033858   24.924293   -4.485634  -24.920159  ...  -88.547933   \n",
       "1  -60.850271  -99.608253 -122.481351 -135.805903  ... -148.102136   \n",
       "2  -42.777537 -152.298608 -216.774954 -237.856332  ...  -49.258874   \n",
       "3 -213.653265 -337.223887 -420.590955 -460.802000  ... -163.775586   \n",
       "4 -239.673435 -305.298202 -338.179450 -333.613276  ...  -66.723384   \n",
       "\n",
       "          740         741         742         743         744         745  \\\n",
       "0  -89.019320  -90.127355  -92.258864  -95.856269 -101.250935 -108.468740   \n",
       "1 -148.435120 -148.071567 -147.175232 -145.993129 -144.716644 -143.564875   \n",
       "2  -51.862350  -55.717751  -60.458831  -65.747069  -71.188346  -76.499629   \n",
       "3 -168.771061 -172.394643 -174.925776 -176.560834 -177.385332 -177.346131   \n",
       "4  -68.065778  -69.814589  -71.802241  -73.916716  -76.018236  -78.105941   \n",
       "\n",
       "          746         747         748         749         750         751  \\\n",
       "0 -117.174545 -126.866650 -136.987933 -146.759194 -155.568074 -163.024622   \n",
       "1 -142.784641 -142.456042 -142.659135 -143.390609 -144.647124 -146.369767   \n",
       "2  -81.453426  -85.877801  -89.600835  -92.506193  -94.588698  -95.954339   \n",
       "3 -176.445863 -174.715135 -172.240285 -169.135580 -165.487644 -161.438765   \n",
       "4  -80.151241  -82.042274  -83.722811  -85.136721  -86.255764  -86.996273   \n",
       "\n",
       "          752         753         754         755         756         757  \\\n",
       "0 -168.905759 -173.044183 -175.189496 -175.147097 -172.722641 -167.666484   \n",
       "1 -148.416286 -150.699990 -153.106431 -155.576752 -158.024374 -160.362790   \n",
       "2  -96.709179  -96.903808  -96.588912  -95.926395  -94.939389  -93.567817   \n",
       "3 -157.131323 -152.624433 -152.350999 -153.520596 -153.995098 -153.693602   \n",
       "4  -88.756240  -94.815196  -98.828187 -101.211730 -102.410073 -102.922988   \n",
       "\n",
       "          758         759         760         761         762         763  \\\n",
       "0 -159.645907 -154.558980 -159.248125 -163.096352 -166.105715 -168.306237   \n",
       "1 -162.533368 -164.422034 -165.748181 -165.786908 -163.480141 -157.908862   \n",
       "2  -91.835068  -89.848001  -87.769175  -85.566848  -83.181642  -82.434643   \n",
       "3 -152.479622 -150.022180 -146.262892 -162.282513 -180.724026 -199.408129   \n",
       "4 -103.222461 -103.419378 -103.569087 -103.671405 -103.670621 -103.455485   \n",
       "\n",
       "          764         765         766         767         768         769  \\\n",
       "0 -169.894790 -171.068419 -172.107669 -173.237675 -174.128149 -173.698911   \n",
       "1 -148.182000 -133.436420 -113.114693 -104.015477 -107.492194 -109.214349   \n",
       "2  -83.447288  -83.885479  -83.390227  -81.269346  -76.330754  -66.632438   \n",
       "3 -218.417858 -237.280836 -255.358154 -271.399919 -284.211912 -291.683349   \n",
       "4 -102.859205 -101.826101 -100.272705 -103.268685 -108.151417 -112.027124   \n",
       "\n",
       "          770         771         772         773         774         775  \\\n",
       "0 -169.453197 -157.755404 -134.192168  -94.738984  -38.149051   33.407889   \n",
       "1 -108.071708 -102.898443  -92.250895  -74.685339  -49.230191  -16.608211   \n",
       "2  -49.260197  -33.407509   -7.343445   34.055886   93.688717  171.635383   \n",
       "3 -291.286863 -280.356260 -255.975375 -215.950270 -158.892523  -85.691418   \n",
       "4 -115.006475 -119.000189 -117.947333 -109.992611 -111.180313 -109.479520   \n",
       "\n",
       "          776         777         778         779         780         781  \\\n",
       "0  113.623386  181.663722  235.362578  263.293168  268.110091  256.595362   \n",
       "1   21.096845   45.353370   64.450345   76.777817   79.225118   69.709449   \n",
       "2  267.982913  375.424092  481.152093  565.983967  600.110386  555.225303   \n",
       "3   -0.013901   91.056566  177.186706  245.821227  283.905058  280.577788   \n",
       "4 -103.079964  -92.174006  -78.565046  -65.306411  -56.868020  -57.275274   \n",
       "\n",
       "          782         783         784         785         786         787  \\\n",
       "0  222.022915  170.403166  109.778496   47.557057   -6.867491  -50.842132   \n",
       "1   48.398096   16.819536  -21.553241  -76.593105 -130.380987 -171.580882   \n",
       "2  446.975084  301.920451  142.840979   -5.483496 -129.118325 -216.127621   \n",
       "3  230.118094  115.698626  -29.526101 -178.822886 -312.327657 -444.057526   \n",
       "4  -70.025732  -97.672458 -143.871003 -237.889201 -327.044970 -396.488424   \n",
       "\n",
       "          788         789         790         791         792         793  \\\n",
       "0  -84.000763 -108.869908 -127.614806 -141.733882 -152.642114 -160.809951   \n",
       "1 -196.858438 -208.845463 -218.906818 -213.228019 -195.891290 -182.338511   \n",
       "2 -258.825272 -264.125043 -252.047729 -225.481853 -195.123063 -168.765509   \n",
       "3 -543.993562 -598.565706 -609.729110 -606.674266 -629.600410 -633.769885   \n",
       "4 -436.288614 -445.749324 -428.424438 -392.701295 -347.828478 -301.888040   \n",
       "\n",
       "          794         795         796         797         798         799  \\\n",
       "0 -166.291106 -169.167031 -169.991375 -169.567777 -168.449881 -167.024682   \n",
       "1 -173.722071 -165.882849 -159.015529 -152.731567 -146.531412 -139.804490   \n",
       "2 -149.308818 -137.736904 -131.929044 -131.678389 -132.410667 -132.846320   \n",
       "3 -626.041381 -611.274067 -591.772063 -566.701128 -535.033109 -495.712601   \n",
       "4 -260.212168 -224.938728 -197.316809 -176.178931 -159.774366 -146.408009   \n",
       "\n",
       "          800         801         802         803         804         805  \\\n",
       "0 -167.883440 -165.360556 -162.664542 -161.491838 -160.475140 -159.585054   \n",
       "1 -132.301418 -123.911774 -114.691859 -104.920245  -99.423865  -97.547836   \n",
       "2 -132.540209 -131.436292 -129.645397 -127.334104 -124.752514 -121.984242   \n",
       "3 -449.018041 -396.228349 -339.706229 -282.787013 -229.000846 -181.155968   \n",
       "4 -134.912591 -132.774873 -130.625636 -127.741452 -124.625525 -121.892214   \n",
       "\n",
       "          806         807         808         809         810         811  \\\n",
       "0 -158.792256 -158.095275 -157.464944 -156.872187 -156.315797 -155.739110   \n",
       "1  -95.313473  -92.386114  -88.431242  -83.253366  -76.712689  -68.475097   \n",
       "2 -119.113074 -116.222951 -113.397960 -110.666769 -107.974831 -105.378817   \n",
       "3 -167.427340 -162.233406 -156.945550 -151.677099 -146.541540 -141.652490   \n",
       "4 -119.905920 -118.975521 -119.409923 -121.406949 -124.914444 -129.685834   \n",
       "\n",
       "          812         813         814         815         816         817  \\\n",
       "0 -155.141124 -154.520945 -153.850016 -153.072119 -152.186716 -151.165610   \n",
       "1  -58.262159  -50.800350  -51.643810  -52.150019  -52.236058  -54.506096   \n",
       "2 -102.852163 -100.312829  -97.678844  -94.868297  -91.827097  -88.646292   \n",
       "3 -137.040335 -132.596639 -128.101888 -123.308791 -117.942240 -111.643717   \n",
       "4 -135.446790 -141.645229 -147.562428 -152.674136 -156.678353 -159.245330   \n",
       "\n",
       "          818         819         820         821         822         823  \\\n",
       "0 -149.869621 -148.104135 -145.702434 -142.497915 -138.185193 -132.792310   \n",
       "1  -59.316539  -62.564915  -64.446109  -65.571776  -66.137005  -68.294204   \n",
       "2  -86.273780  -83.888295  -81.463796  -78.974204  -76.337838  -73.528520   \n",
       "3 -104.137927  -95.371647  -85.513697  -74.788242  -63.391446  -51.658117   \n",
       "4 -160.156457 -159.470933 -157.359091 -153.907951 -149.148996 -143.224828   \n",
       "\n",
       "          824         825         826         827         828         829  \\\n",
       "0 -126.569616 -119.878641 -113.108747 -107.087917 -102.548624  -99.854653   \n",
       "1  -85.047368  -87.188405  -79.661667  -67.967300  -58.446898  -55.365442   \n",
       "2  -70.520010  -67.369340  -64.105691  -60.785953  -57.466935  -54.205371   \n",
       "3  -39.950605  -28.519909  -17.450140   -6.741875    3.548940    9.319677   \n",
       "4 -136.250270 -128.229027 -128.250634 -128.076160 -127.551444 -126.650546   \n",
       "\n",
       "          830         831         832         833         834         835  \\\n",
       "0  -99.559947  -98.597387  -96.968555  -94.647343  -91.635502  -88.018201   \n",
       "1  -51.232145  -45.770655  -38.954670  -34.484791  -36.612438  -39.859366   \n",
       "2  -51.057910  -48.025567  -45.137050  -42.337649  -39.628132  -36.981402   \n",
       "3   15.056802   22.137801   30.811861   41.217028   53.130239   66.217340   \n",
       "4 -125.375316 -123.727603 -121.681465 -119.266476 -116.039941 -110.920200   \n",
       "\n",
       "          836        837        838        839        840        841  \\\n",
       "0  -83.964022 -79.447179 -75.512136 -73.126446 -70.294428 -66.570445   \n",
       "1  -43.838695 -48.080378 -51.975631 -55.165819 -57.486888 -58.913811   \n",
       "2  -34.342511 -31.711983 -29.241329 -27.077279 -26.860028 -26.819392   \n",
       "3   80.088677  74.736841  65.604044  56.567211  47.433238  37.953482   \n",
       "4 -102.464393 -88.729544 -67.383656 -36.261235   4.497835  52.364989   \n",
       "\n",
       "         842        843        844        845        846        847  \\\n",
       "0 -61.564506 -54.692266 -45.119472 -32.400846 -16.507860 -38.575326   \n",
       "1 -59.505018 -59.402393 -58.886826 -58.155982 -57.324295 -56.450749   \n",
       "2 -26.955640 -27.185669 -27.481906 -27.761196 -27.912589 -27.797346   \n",
       "3  27.962645  17.128758   5.286510  -7.534992 -21.140009 -36.671868   \n",
       "4  64.680431  66.821965  67.103166  65.468440  61.862411  56.313237   \n",
       "\n",
       "         848        849         850         851         852         853  \\\n",
       "0 -62.477736 -83.417822 -100.951659 -115.163223 -126.553296 -135.539485   \n",
       "1 -55.511086 -54.481144  -53.309075  -51.943117  -50.359373  -48.534031   \n",
       "2 -27.332278 -26.461974  -25.131027  -23.339598  -21.115640  -18.570461   \n",
       "3 -53.222370 -69.121627  -84.507556  -99.351622 -113.625523 -126.967871   \n",
       "4   8.500049 -82.185010 -167.258829 -232.831355 -270.373622 -277.745567   \n",
       "\n",
       "          854         855         856         857         858         859  \\\n",
       "0 -142.817345 -149.082610 -154.558980 -159.248125 -163.096352 -166.105715   \n",
       "1  -46.471138  -44.202600  -41.760400  -39.148818  -36.372204  -33.434976   \n",
       "2  -15.787618  -12.822919   -9.759984   -6.626917   -3.479638   -0.290772   \n",
       "3 -138.878652 -148.997014 -157.240162 -163.775586 -168.771061 -172.394643   \n",
       "4 -259.140503 -223.946256 -182.578510 -143.453039 -111.652376  -88.564698   \n",
       "\n",
       "          860         861         862         863         864         865  \\\n",
       "0 -168.306237 -169.894790 -171.068419 -172.107669 -173.237675 -174.128149   \n",
       "1  -30.341614  -27.041100  -27.289439  -44.063488  -57.971507  -68.596745   \n",
       "2    2.967015    6.265465    9.576287   10.160019   10.727593   10.919412   \n",
       "3 -174.925776 -176.560834 -177.385332 -177.346131 -176.445863 -174.715135   \n",
       "4  -76.367330  -81.444609  -85.443985  -88.446869  -90.645773  -94.957192   \n",
       "\n",
       "          866         867         868         869         870         871  \\\n",
       "0 -173.698911 -169.453197 -157.755404 -134.192168  -94.738984  -38.149051   \n",
       "1  -75.996953  -85.460687  -93.506936 -100.246186 -105.816716 -110.384600   \n",
       "2   11.153538   10.902894   10.707506   10.714524   10.784154   10.915497   \n",
       "3 -172.240285 -169.135580 -165.487644 -161.438765 -157.131323 -152.624433   \n",
       "4  -98.665013 -100.972119 -102.046908 -102.058014 -101.396533 -100.287119   \n",
       "\n",
       "          872         873         874         875         876         877  \\\n",
       "0   33.407889   26.944514    6.637850  -13.533267  -32.927279  -50.291455   \n",
       "1 -114.338159 -117.871305 -121.039092 -123.841053 -126.276759 -128.429150   \n",
       "2   11.079883   11.276442   11.504334   11.790535   12.162067   12.562678   \n",
       "3 -147.977261 -145.401889 -145.015917 -144.483418 -143.668490 -144.000700   \n",
       "4 -106.110126 -112.042856 -117.101183 -121.036711 -123.767972 -125.352643   \n",
       "\n",
       "          878         879         880         881         882         883  \\\n",
       "0  -64.817480  -76.252607  -84.538581  -89.867225  -92.736037  -94.114876   \n",
       "1 -130.325643 -131.938135 -133.210772 -134.087729 -134.907924 -135.454496   \n",
       "2   12.963962   13.282029   13.405298   12.888943   11.371571    8.714115   \n",
       "3 -162.282513 -180.724026 -199.408129 -218.417858 -237.280836 -255.358154   \n",
       "4 -125.904210 -125.564187 -124.335442 -122.248856 -122.076966 -121.966282   \n",
       "\n",
       "          884         885         886         887         888         889  \\\n",
       "0  -94.640438  -94.644052 -108.869908 -127.614806 -141.733882 -152.642114   \n",
       "1 -134.930457 -133.115240 -130.418238 -127.916551 -131.260199 -135.290131   \n",
       "2    4.860948   -0.243444   -6.487791  -13.696223  -22.591176  -33.367569   \n",
       "3 -271.399919 -284.211912 -291.683349 -291.286863 -311.933819 -412.791517   \n",
       "4 -121.444663 -120.513386 -119.173847 -117.455330 -115.415004 -113.054584   \n",
       "\n",
       "          890         891         892         893         894         895  \\\n",
       "0 -160.809951 -166.291106 -169.167031 -169.991375 -169.567777 -168.449881   \n",
       "1 -138.993104 -142.396642 -145.639307 -148.804036 -151.918142 -154.981095   \n",
       "2  -43.377934  -52.989372  -62.541103  -72.400035  -82.738539  -93.701126   \n",
       "3 -476.535978 -557.466529 -606.674266 -629.600410 -633.769885 -626.041381   \n",
       "4 -110.431436 -107.575237 -104.515748 -101.227250  -98.844753  -97.582131   \n",
       "\n",
       "          896         897         898         899         900        901  \\\n",
       "0 -167.024682 -165.512534 -164.022713 -162.664542   42.178220 -69.494141   \n",
       "1 -157.964530 -160.951137 -163.940223 -166.931056   45.600836 -58.337274   \n",
       "2 -105.209999 -116.992841 -128.777253 -140.290755   39.876474 -58.153225   \n",
       "3 -611.274067 -591.772063 -566.701128 -587.411075  146.331552 -80.614658   \n",
       "4  -95.700269  -93.225825  -91.959060  -92.619381   67.107476 -66.970043   \n",
       "\n",
       "          902         903         904         905          906         907  \\\n",
       "0 -107.674375  198.932065  360.369182  815.846154   817.538462  186.931217   \n",
       "1 -135.289086  116.187169  155.098157  789.047619   811.568627  167.333333   \n",
       "2 -233.394404   59.456172  647.143129  900.689655   900.344828  182.111111   \n",
       "3 -452.181306   79.535368  439.802266  406.615385  2004.871795  171.111111   \n",
       "4 -320.795351  176.213541  178.868548  612.424242   626.589147  146.969697   \n",
       "\n",
       "          908         909        910         911         912        913  \\\n",
       "0  121.767677  117.070707  29.378788   73.708605   33.449697 -66.502132   \n",
       "1  132.190476  135.238095  32.142857   73.846890   43.067625 -58.336062   \n",
       "2  119.444444   86.000000  39.966667   68.782483   40.210325 -60.883929   \n",
       "3  131.666667   96.190476  33.785714  158.285005  184.039253 -77.021869   \n",
       "4  120.378788   98.636364  27.840909   99.131894   66.603497 -71.033316   \n",
       "\n",
       "          914         915         916         917          918         919  \\\n",
       "0 -104.719503  196.101072  356.351168  830.000000   823.333333  173.333333   \n",
       "1 -137.640020  113.876522  140.949618  800.000000   786.666667  173.333333   \n",
       "2 -239.928115   53.003391  645.257053  906.666667   903.333333  183.333333   \n",
       "3 -448.663312  -36.750449  437.976693  383.333333  1656.666667  186.666667   \n",
       "4 -327.652237  174.391177  169.697309  600.000000   603.333333  168.333333   \n",
       "\n",
       "          920         921   922         923         924        925        926  \\\n",
       "0  126.666667   91.666667  33.5   72.289157   45.665314  23.252994  18.778421   \n",
       "1  123.333333  110.000000  35.0   71.713147   36.216761  27.299799  62.163862   \n",
       "2  108.333333   63.333333  40.0   70.588235   25.344543  17.332724  32.532097   \n",
       "3  146.666667  100.000000  40.0  166.666667  111.562434  44.520546  54.550278   \n",
       "4  110.000000   86.666667  34.5  100.000000   44.155314  38.968145  47.241720   \n",
       "\n",
       "          927        928        929         930        931        932  \\\n",
       "0   25.495103  46.431980  65.387813   96.618515  72.145995  42.633114   \n",
       "1   54.910037  40.242121  51.551001  174.405124  31.860808  45.662272   \n",
       "2   35.313818  30.164765  35.291662   95.457538   4.673276  42.138074   \n",
       "3  214.536985  51.869951  70.178021  973.407633  24.393887  44.716923   \n",
       "4   36.112023  82.853353  69.155671  117.031519  35.354040  31.319597   \n",
       "\n",
       "         933        934        935         936        937         938  \\\n",
       "0  75.088086  10.046506   8.510272  273.236801 -38.207580  -63.517605   \n",
       "1  76.562188   8.986157   4.067595  138.536977 -10.137490  159.419397   \n",
       "2  75.750076   1.277585   2.604269  109.443646   4.679702  -74.585329   \n",
       "3  26.721882   9.274994  15.935667  340.851880  26.990479 -330.096571   \n",
       "4  43.107325  10.293297   6.087403  191.351890  80.127998 -128.076160   \n",
       "\n",
       "          939         940         941          942         943         944  \\\n",
       "0  298.723811  535.188836  853.333333  1050.000000  413.333333  333.333333   \n",
       "1  341.411772  231.478680  863.333333  1590.000000  226.666667  286.666667   \n",
       "2  229.882472  702.155116  993.333333  1206.666667  190.000000  250.000000   \n",
       "3  488.233931  582.207504  676.666667  4160.000000  190.000000  183.333333   \n",
       "4  240.696324  488.124756  950.000000  1243.333333  216.666667  190.000000   \n",
       "\n",
       "          945   946         947        948         949         950  \\\n",
       "0  376.666667  44.0  185.567010 -41.018732 -175.147097 -169.567777   \n",
       "1  343.333333  51.0  102.857143 -94.812612 -165.786908 -218.906818   \n",
       "2  353.333333  43.0   71.428571 -49.105674  -96.903808 -264.125043   \n",
       "3  150.000000  43.0  178.217822 -56.712875 -291.286863 -626.041381   \n",
       "4  230.000000  41.0  117.647059 -65.650306 -117.947333 -428.424438   \n",
       "\n",
       "          951         952         953         954         955        956  \\\n",
       "0  136.528583  268.110091  323.333333  240.000000   73.333333  53.333333   \n",
       "1   -7.091954   79.225118  583.333333  530.000000  103.333333  56.666667   \n",
       "2   24.672119  600.110386  840.000000  693.333333  166.666667  76.666667   \n",
       "3 -113.862059  283.905058  336.666667  450.000000  136.666667  50.000000   \n",
       "4   66.821965  -56.868020  510.000000  553.333333   96.666667  83.333333   \n",
       "\n",
       "         957   958        959         960        961  962  963  964  965  966  \\\n",
       "0  23.333333  11.0  70.312500  815.846154  65.896675  NaN  NaN  NaN  NaN  NaN   \n",
       "1  33.333333  15.0  69.498069  789.047619  52.303610  NaN  NaN  NaN  NaN  NaN   \n",
       "2  33.333333  36.0  60.402685  900.689655  35.916341  NaN  NaN  NaN  NaN  NaN   \n",
       "3  36.666667  16.0  88.669951  406.615385  70.724162  NaN  NaN  NaN  NaN  NaN   \n",
       "4  43.333333  12.0  63.157895  612.424242  69.955185  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   967        968        969       970       971         972     973  \\\n",
       "0  NaN  64.209942  64.223760  0.080771  0.078703  830.000000  14.826   \n",
       "1  NaN  55.209050  56.032151  0.066287  0.069969  800.000000  44.478   \n",
       "2  NaN  26.532400  26.862158  0.039876  0.029458  906.666667  29.652   \n",
       "3  NaN  93.910973  94.652650  0.173934  0.230958  383.333333  34.594   \n",
       "4  NaN  83.534641  84.522923  0.114227  0.136400  600.000000  22.239   \n",
       "\n",
       "        974        975        976        977       978       979  980  981  \\\n",
       "0  0.017863  23.333333   1.538462   4.615385  3.823529  523.4375  NaN  NaN   \n",
       "1  0.055597  63.333333   8.571429  68.571429  8.750000  226.5625  NaN  NaN   \n",
       "2  0.032704  46.666667   3.448276  51.724138  4.142857   78.1250  NaN  NaN   \n",
       "3  0.090245  50.000000  32.307692  63.076923  5.000000   31.2500  NaN  NaN   \n",
       "4  0.037065  30.833333   9.090909  25.000000  6.285714  109.3750  NaN  NaN   \n",
       "\n",
       "        982       983       984       985       986       987       988  \n",
       "0  0.006599  0.004682  0.000334  1.409581  0.568152  0.403065 -5.364064  \n",
       "1       NaN  0.024982  0.010756       NaN       NaN  0.699037 -3.689603  \n",
       "2       NaN  0.056782  0.000642       NaN       NaN  0.988822 -2.868542  \n",
       "3       NaN  0.120121  0.058922       NaN       NaN  0.670907 -2.119254  \n",
       "4       NaN  0.033902  0.005670       NaN       NaN  0.856713 -3.384267  \n",
       "\n",
       "[5 rows x 989 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61bc1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full['NaNs'] = df_train_full.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48207245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full['NaN_perc'] = df_train_full.NaNs/df_train_full.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86463273",
   "metadata": {},
   "source": [
    "only one sample with almost all Nans -> remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9cf54f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011111    4017\n",
       "0.008081     742\n",
       "0.015152     172\n",
       "0.012121     113\n",
       "0.089899      29\n",
       "0.014141      21\n",
       "0.016162       9\n",
       "0.021212       8\n",
       "0.013131       5\n",
       "0.998990       1\n",
       "Name: NaN_perc, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_full['NaN_perc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b60b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = XGBClassifier(learning_rate=0.05, n_estimators=300, max_depth=5)\n",
    "\n",
    "clf2 = GradientBoostingClassifier(learning_rate=0.05, n_estimators=500, max_depth=7, \n",
    "                                 min_samples_split=60, min_samples_leaf=9, subsample=1.0,\n",
    "                                 max_features=50, random_state=0)\n",
    "\n",
    "param_grid = {\n",
    "    'imputer': [SimpleImputer(strategy='constant', fill_value=0, add_indicator=True)],\n",
    "    'scaler': [RobustScaler()],\n",
    "    'selector': [SelectKBest()],\n",
    "    'selector__k': [5, 10, 50],\n",
    "    'pca': [KernelPCA()],\n",
    "    'pca__n_components': [5, 10, 50],\n",
    "    'clf': [clf1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add VarianceThreshold, we got constant features (nan to zero)\n",
    "#962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fbfa4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('threshold', VarianceThreshold()),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('pca', KernelPCA()),\n",
    "    ('clf', XGBClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9ece5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='f1_micro', cv=5, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d213df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:55:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 1/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.688 total time=  58.8s\n",
      "[CV 2/5; 1/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1051 1052 1053 1054 1055 1056\n",
      " 1069 1070] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:56:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 1/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.680 total time= 1.4min\n",
      "[CV 3/5; 1/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:57:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 1/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.701 total time= 1.4min\n",
      "[CV 4/5; 1/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:59:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 1/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.709 total time= 1.5min\n",
      "[CV 5/5; 1/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:00:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 1/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.709 total time= 1.4min\n",
      "[CV 1/5; 2/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:02:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 2/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.674 total time= 1.5min\n",
      "[CV 2/5; 2/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1051 1052 1053 1054 1055 1056\n",
      " 1069 1070] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:03:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 2/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.703 total time= 1.3min\n",
      "[CV 3/5; 2/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:04:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 2/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.694 total time= 1.2min\n",
      "[CV 4/5; 2/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:06:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 2/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.664 total time=  58.0s\n",
      "[CV 5/5; 2/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:07:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 2/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.663 total time= 1.2min\n",
      "[CV 1/5; 3/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:08:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 3/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50;, score=0.652 total time= 1.4min\n",
      "[CV 2/5; 3/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1051 1052 1053 1054 1055 1056\n",
      " 1069 1070] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:09:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 3/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50;, score=0.631 total time= 1.4min\n",
      "[CV 3/5; 3/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:11:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 3/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50;, score=0.642 total time= 1.9min\n",
      "[CV 4/5; 3/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:12:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 3/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50;, score=0.646 total time= 1.2min\n",
      "[CV 5/5; 3/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:14:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 3/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=5, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50;, score=0.641 total time= 1.6min\n",
      "[CV 1/5; 4/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:16:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 4/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.688 total time= 2.6min\n",
      "[CV 2/5; 4/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1051 1052 1053 1054 1055 1056\n",
      " 1069 1070] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:19:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 4/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.680 total time= 3.0min\n",
      "[CV 3/5; 4/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:22:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 4/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.701 total time= 3.0min\n",
      "[CV 4/5; 4/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:28:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 4/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.709 total time= 5.8min\n",
      "[CV 5/5; 4/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:34:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 4/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=5;, score=0.709 total time= 5.7min\n",
      "[CV 1/5; 5/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:40:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 5/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.704 total time= 9.1min\n",
      "[CV 2/5; 5/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1051 1052 1053 1054 1055 1056\n",
      " 1069 1070] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:51:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 5/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.732 total time=11.2min\n",
      "[CV 3/5; 5/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:03:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 5/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.720 total time=11.9min\n",
      "[CV 4/5; 5/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:13:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 5/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.718 total time= 9.6min\n",
      "[CV 5/5; 5/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:21:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 5/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=10;, score=0.719 total time= 7.8min\n",
      "[CV 1/5; 6/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1951 1952 1953 1954 1955 1956\n",
      " 1969 1970] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:29:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 6/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50;, score=0.678 total time=10.5min\n",
      "[CV 2/5; 6/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 962  963  964  965  966  967  980  981 1051 1052 1053 1054 1055 1056\n",
      " 1069 1070] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/home/rapwag01/.virtualenvs/vethaml/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:40:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 6/9] END clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50;, score=0.692 total time=10.2min\n",
      "[CV 3/5; 6/9] START clf=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), imputer=SimpleImputer(add_indicator=True, fill_value=0, strategy='constant'), pca=KernelPCA(), pca__n_components=10, scaler=RobustScaler(), selector=SelectKBest(), selector__k=50\n"
     ]
    }
   ],
   "source": [
    "grid.fit(X_train_full, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd43523f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', RobustScaler()),\n",
       "                ('selector', SelectKBest(k=100)),\n",
       "                ('pca', KernelPCA(n_components=50)),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, enable_categorical=False,\n",
       "                               gamma=0, gpu_id=-1, importance_type=None,\n",
       "                               interaction_constraints='', learning_rate=0.05,\n",
       "                               max_delta_step=0, max_depth=7,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=500,\n",
       "                               n_jobs=4, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=None, subsample=1,\n",
       "                               tree_method='exact', validate_parameters=1,\n",
       "                               verbosity=None))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6589221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.13453327814737"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total fit time in min\n",
    "grid.cv_results_['mean_fit_time'].sum()/60+grid.refit_time_/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f420034b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([355.32276225, 352.91089821, 360.78657513, 446.23621502,\n",
       "        542.56032243, 492.97693124]),\n",
       " 'std_fit_time': array([ 7.37814669,  2.24550966,  3.57112282,  3.5199128 ,  2.80051938,\n",
       "        35.66951078]),\n",
       " 'mean_score_time': array([0.36865253, 0.2251111 , 0.27229733, 0.24745607, 0.3048562 ,\n",
       "        0.29407144]),\n",
       " 'std_score_time': array([0.15649779, 0.02188388, 0.03999984, 0.02521694, 0.03070745,\n",
       "        0.02930795]),\n",
       " 'param_clf': masked_array(data=[XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                                  colsample_bynode=None, colsample_bytree=None,\n",
       "                                  enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                                  importance_type=None, interaction_constraints=None,\n",
       "                                  learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                                  min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                                  predictor=None, random_state=None, reg_alpha=None,\n",
       "                                  reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                                  tree_method=None, validate_parameters=None, verbosity=None)   ,\n",
       "                    XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                                  colsample_bynode=None, colsample_bytree=None,\n",
       "                                  enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                                  importance_type=None, interaction_constraints=None,\n",
       "                                  learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                                  min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                                  predictor=None, random_state=None, reg_alpha=None,\n",
       "                                  reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                                  tree_method=None, validate_parameters=None, verbosity=None)   ,\n",
       "                    XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                                  colsample_bynode=None, colsample_bytree=None,\n",
       "                                  enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                                  importance_type=None, interaction_constraints=None,\n",
       "                                  learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                                  min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                                  predictor=None, random_state=None, reg_alpha=None,\n",
       "                                  reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                                  tree_method=None, validate_parameters=None, verbosity=None)   ,\n",
       "                    XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                                  colsample_bynode=None, colsample_bytree=None,\n",
       "                                  enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                                  importance_type=None, interaction_constraints=None,\n",
       "                                  learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                                  min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                                  predictor=None, random_state=None, reg_alpha=None,\n",
       "                                  reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                                  tree_method=None, validate_parameters=None, verbosity=None)   ,\n",
       "                    XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                                  colsample_bynode=None, colsample_bytree=None,\n",
       "                                  enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                                  importance_type=None, interaction_constraints=None,\n",
       "                                  learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                                  min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                                  predictor=None, random_state=None, reg_alpha=None,\n",
       "                                  reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                                  tree_method=None, validate_parameters=None, verbosity=None)   ,\n",
       "                    XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                                  colsample_bynode=None, colsample_bytree=None,\n",
       "                                  enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                                  importance_type=None, interaction_constraints=None,\n",
       "                                  learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                                  min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                                  predictor=None, random_state=None, reg_alpha=None,\n",
       "                                  reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                                  tree_method=None, validate_parameters=None, verbosity=None)   ],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__learning_rate': masked_array(data=[0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__max_depth': masked_array(data=[7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__n_estimators': masked_array(data=[500, 500, 500, 500, 500, 500],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_imputer': masked_array(data=[SimpleImputer(), SimpleImputer(), SimpleImputer(),\n",
       "                    SimpleImputer(), SimpleImputer(), SimpleImputer()],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_pca': masked_array(data=[KernelPCA(n_components=50), KernelPCA(n_components=50),\n",
       "                    KernelPCA(n_components=50), KernelPCA(n_components=50),\n",
       "                    KernelPCA(n_components=50), KernelPCA(n_components=50)],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_pca__n_components': masked_array(data=[50, 50, 50, 400, 400, 400],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_scaler': masked_array(data=[RobustScaler(), RobustScaler(), RobustScaler(),\n",
       "                    RobustScaler(), RobustScaler(), RobustScaler()],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_selector': masked_array(data=[SelectKBest(k=100), SelectKBest(k=100),\n",
       "                    SelectKBest(k=100), SelectKBest(k=100),\n",
       "                    SelectKBest(k=100), SelectKBest(k=100)],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_selector__k': masked_array(data=[100, 300, 500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf': XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                 colsample_bynode=None, colsample_bytree=None,\n",
       "                 enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                 importance_type=None, interaction_constraints=None,\n",
       "                 learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                 n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                 tree_method=None, validate_parameters=None, verbosity=None),\n",
       "   'clf__learning_rate': 0.05,\n",
       "   'clf__max_depth': 7,\n",
       "   'clf__n_estimators': 500,\n",
       "   'imputer': SimpleImputer(),\n",
       "   'pca': KernelPCA(n_components=50),\n",
       "   'pca__n_components': 50,\n",
       "   'scaler': RobustScaler(),\n",
       "   'selector': SelectKBest(k=100),\n",
       "   'selector__k': 100},\n",
       "  {'clf': XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                 colsample_bynode=None, colsample_bytree=None,\n",
       "                 enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                 importance_type=None, interaction_constraints=None,\n",
       "                 learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                 n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                 tree_method=None, validate_parameters=None, verbosity=None),\n",
       "   'clf__learning_rate': 0.05,\n",
       "   'clf__max_depth': 7,\n",
       "   'clf__n_estimators': 500,\n",
       "   'imputer': SimpleImputer(),\n",
       "   'pca': KernelPCA(n_components=50),\n",
       "   'pca__n_components': 50,\n",
       "   'scaler': RobustScaler(),\n",
       "   'selector': SelectKBest(k=100),\n",
       "   'selector__k': 300},\n",
       "  {'clf': XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                 colsample_bynode=None, colsample_bytree=None,\n",
       "                 enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                 importance_type=None, interaction_constraints=None,\n",
       "                 learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                 n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                 tree_method=None, validate_parameters=None, verbosity=None),\n",
       "   'clf__learning_rate': 0.05,\n",
       "   'clf__max_depth': 7,\n",
       "   'clf__n_estimators': 500,\n",
       "   'imputer': SimpleImputer(),\n",
       "   'pca': KernelPCA(n_components=50),\n",
       "   'pca__n_components': 50,\n",
       "   'scaler': RobustScaler(),\n",
       "   'selector': SelectKBest(k=100),\n",
       "   'selector__k': 500},\n",
       "  {'clf': XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                 colsample_bynode=None, colsample_bytree=None,\n",
       "                 enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                 importance_type=None, interaction_constraints=None,\n",
       "                 learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                 n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                 tree_method=None, validate_parameters=None, verbosity=None),\n",
       "   'clf__learning_rate': 0.05,\n",
       "   'clf__max_depth': 7,\n",
       "   'clf__n_estimators': 500,\n",
       "   'imputer': SimpleImputer(),\n",
       "   'pca': KernelPCA(n_components=50),\n",
       "   'pca__n_components': 400,\n",
       "   'scaler': RobustScaler(),\n",
       "   'selector': SelectKBest(k=100),\n",
       "   'selector__k': 100},\n",
       "  {'clf': XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                 colsample_bynode=None, colsample_bytree=None,\n",
       "                 enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                 importance_type=None, interaction_constraints=None,\n",
       "                 learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                 n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                 tree_method=None, validate_parameters=None, verbosity=None),\n",
       "   'clf__learning_rate': 0.05,\n",
       "   'clf__max_depth': 7,\n",
       "   'clf__n_estimators': 500,\n",
       "   'imputer': SimpleImputer(),\n",
       "   'pca': KernelPCA(n_components=50),\n",
       "   'pca__n_components': 400,\n",
       "   'scaler': RobustScaler(),\n",
       "   'selector': SelectKBest(k=100),\n",
       "   'selector__k': 300},\n",
       "  {'clf': XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "                 colsample_bynode=None, colsample_bytree=None,\n",
       "                 enable_categorical=False, gamma=None, gpu_id=None,\n",
       "                 importance_type=None, interaction_constraints=None,\n",
       "                 learning_rate=0.05, max_delta_step=None, max_depth=7,\n",
       "                 min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                 n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "                 tree_method=None, validate_parameters=None, verbosity=None),\n",
       "   'clf__learning_rate': 0.05,\n",
       "   'clf__max_depth': 7,\n",
       "   'clf__n_estimators': 500,\n",
       "   'imputer': SimpleImputer(),\n",
       "   'pca': KernelPCA(n_components=50),\n",
       "   'pca__n_components': 400,\n",
       "   'scaler': RobustScaler(),\n",
       "   'selector': SelectKBest(k=100),\n",
       "   'selector__k': 500}],\n",
       " 'split0_test_score': array([0.75457875, 0.73137973, 0.75213675, 0.75946276, 0.74725275,\n",
       "        0.73626374]),\n",
       " 'split1_test_score': array([0.72405372, 0.73382173, 0.72161172, 0.71672772, 0.73137973,\n",
       "        0.73260073]),\n",
       " 'split2_test_score': array([0.76190476, 0.72893773, 0.72283272, 0.74725275, 0.72405372,\n",
       "        0.74847375]),\n",
       " 'split3_test_score': array([0.74449878, 0.75183374, 0.75305623, 0.74205379, 0.74694377,\n",
       "        0.73471883]),\n",
       " 'split4_test_score': array([0.72616137, 0.73716381, 0.72493888, 0.72127139, 0.73471883,\n",
       "        0.71882641]),\n",
       " 'mean_test_score': array([0.74223948, 0.73662735, 0.73491526, 0.73735368, 0.73686976,\n",
       "        0.73417669]),\n",
       " 'std_test_score': array([0.01505532, 0.00807541, 0.01447878, 0.01608082, 0.0090367 ,\n",
       "        0.00945696]),\n",
       " 'rank_test_score': array([1, 4, 5, 2, 3, 6], dtype=int32)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0cd51d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', RobustScaler()),\n",
       "                ('selector', SelectKBest(k=100)),\n",
       "                ('pca', KernelPCA(n_components=50)),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, enable_categorical=False,\n",
       "                               gamma=0, gpu_id=-1, importance_type=None,\n",
       "                               interaction_constraints='', learning_rate=0.05,\n",
       "                               max_delta_step=0, max_depth=7,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=500,\n",
       "                               n_jobs=4, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=None, subsample=1,\n",
       "                               tree_method='exact', validate_parameters=1,\n",
       "                               verbosity=None))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b9e2ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7422394774473015"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8a582ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.best_estimator_.predict(X_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d7199c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'results_first_run_partdata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7489b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({'id': np.arange(len(y_pred)), 'y': y_pred.ravel()})\n",
    "df_res.to_csv(f'submissions/{file_name}', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
