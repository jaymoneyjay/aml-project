{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Try Unet\n",
    "\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start simple with provided segmentation models from pytorch\n",
    "https://github.com/qubvel/segmentation_models.pytorch\n",
    "\n",
    "also https://amaarora.github.io/2020/09/13/unet.html\n",
    "and https://github.com/amaarora/amaarora.github.io/blob/master/nbs/Training.ipynb\n",
    "\n",
    "https://www.pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task3.utils.utils import init\n",
    "from task3.utils.config import get_data_loader, get_optimizer, get_model\n",
    "from task3.utils.data_utils import evaluate\n",
    "from task3.utils.img_utils import show_img_batch\n",
    "import importlib\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# PyTorch model and training necessities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import IoU\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Image datasets and image manipulation\n",
    "import torchvision\n",
    "\n",
    "# Image display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = init(config='configs/raphaela.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_name': 'resnet34',\n",
       " 'encoder_weights': None,\n",
       " 'in_channels': 1,\n",
       " 'classes': 2,\n",
       " 'encoder_depth': 5,\n",
       " 'decoder_use_batchnorm': True,\n",
       " 'decoder_channels': [256, 128, 64, 32, 16],\n",
       " 'decoder_attention_type': None,\n",
       " 'activation': 'sigmoid',\n",
       " 'aux_params': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['model'].get('smp-unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(**config['model'].get('smp-unet')).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-19T12:41:03.650150+0000 DEBUG Exclude samples: None, include samples: None, applied transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "2021-12-19T12:41:12.309960+0000 DEBUG Loaded samples: ['DY7ASN54S9', 'GC4N2W3CPZ', 'T8KF3Q2B2F', 'DTKPN7XP6M', 'K3AM1YOI38', 'QPQSQTSI7D', 'D29HWPRZUW', 'QFGAOZX5JI', 'RC4B5W3ATB', '8G7XRFIWB3', 'LDBF4IFZ6H', 'AYPVNPO9R0', 'ZSLPMQEWOY', 'FIW33CVMV0', 'IJZFZ2P3UT', '7V9NY3JFPX', 'YTEBHX6SH5', '5NJTVZ6ZOJ', '7IBG80WR2T', 'VVCUF1AI87', 'R5OV0RKYDG', 'YSCCEISFRH', 'MW16WM2HPQ', '1D7PVKV2AP', '8AHY2IWS17', '4XBC2X5ZTR', 'D47OR19ANJ', 'T2LZGPQNQI', 'VI47TJR3OQ', 'GF7SRBBU1E', '2O92K3GBLM', '0S1GBHENTP', 'M1FI0BAOWB', 'O26IJHI6MH', 'IJN60PIITZ', 'XBTT6NEY4T', 'D8I212VOCH', '6I8D4BKPUE', 'IMNKTJV3OI', 'GFOJ0HGKZH', 'YF6QEAWJXY', 'DMKG04F0SB', 'BBB0U709D2', 'C6WKLGHUIM', '3YLPDRY0FA', '4W4P3UREMQ', '11NZLRCO13', 'QZA3WA0E2X', 'R82Q2AL9P7', '1XHV0Q88M5', 'E9AHVWGBUF', 'WO0OOZLXP6', 'ZMJIDN442P', 'OYZFKJR8U3', '3DGDHONGJW', 'YEZ7BPLZW0', 'W00G0Q112W', 'JANFS05F33', 'ONA22CCCFQ', 'EH667Z5JMT', '55M79ZANVX', 'H7G0BX4HFV', '571G03ZYDA', 'MHD497XXNA', 'TFDB2R0ZSA']\n"
     ]
    }
   ],
   "source": [
    "training_loader, validation_loader = get_data_loader(config, mode='train', get_subset=False)\n",
    "# TODO fix error key\n",
    "# test_loader = get_data_loader(config, mode='test', get_subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_valid = next(iter(validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JANFS05F33_11',\n",
       " 'YEZ7BPLZW0_34',\n",
       " 'QPQSQTSI7D_145',\n",
       " 'W00G0Q112W_73',\n",
       " 'BBB0U709D2_121',\n",
       " 'T2LZGPQNQI_36',\n",
       " 'OYZFKJR8U3_48',\n",
       " 'BBB0U709D2_16',\n",
       " 'E9AHVWGBUF_6',\n",
       " 'FIW33CVMV0_49']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4XBC2X5ZTR_47',\n",
       " 'D29HWPRZUW_83',\n",
       " 'QPQSQTSI7D_67',\n",
       " 'H7G0BX4HFV_28',\n",
       " 'T2LZGPQNQI_101',\n",
       " '1D7PVKV2AP_0',\n",
       " 'GFOJ0HGKZH_0',\n",
       " '0S1GBHENTP_15',\n",
       " 'RC4B5W3ATB_0',\n",
       " 'O26IJHI6MH_183']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_valid['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model.\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use config file\n",
    "# get_model(config)\n",
    "# optimizer = get_optimizer(model, config)\n",
    "# criterion = get_loss(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = config['training']['optimizer']\n",
    "#lr = config['training']['lr']\n",
    "#momentum = config['training']['momentum']\n",
    "#loss = config['training']['loss']\n",
    "#num_classes = config['training'].get('get_classes', 2)\n",
    "#\n",
    "#if optimizer == 'Adam':\n",
    "#    \n",
    "#elif optimizer == 'SGD':\n",
    "#    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "#else:\n",
    "#    raise Exception('Optimizer not supported, choose among Adam and SGD.')\n",
    "#\n",
    "##if loss == 'iou':\n",
    "#    criterion = IoU(num_classes=num_classes)\n",
    "#\n",
    "#elif loss == 'bcewithlogits':\n",
    "#    assert config['model']['smp-unet']['activation'] == None, f'Last layer of the Unet model should not be sigmoid \\\n",
    "#    if you are using {loss}.'\n",
    "#    # Sigmoid + BCELoss, numerically more stable \n",
    "#    criterion = BCEWithLogitsLoss(pos_weight=None) \n",
    "#\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "loss = smp.utils.losses.DiceLoss()    \n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show images on Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# Extract a batch of 4 images\n",
    "dataiter = iter(training_loader)\n",
    "batch = dataiter.next()\n",
    "images, labels = batch['frame_cropped'], batch['label_cropped']\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "\n",
    "# Default log_dir argument is 'runs' - but it's good to be specific\n",
    "# torch.utils.tensorboard.SummaryWriter is imported above\n",
    "writer = SummaryWriter('runs/mv_experiment_1')\n",
    "\n",
    "# Write image data to TensorBoard log dir\n",
    "writer.add_image('Batch Images MV', img_grid)\n",
    "writer.flush()\n",
    "\n",
    "# To view, start TensorBoard on the command line with:\n",
    "#   tensorboard --logdir=runs\n",
    "# ...and open a browser tab to http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[INFO] training the network...\n",
      "epoch 0 / 1\n",
      "torch.Size([10, 1, 30, 40])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 4 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_476204/2123012519.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/vethaml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/vethaml/lib/python3.8/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/vethaml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/vethaml/lib/python3.8/site-packages/segmentation_models_pytorch/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/vethaml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/vethaml/lib/python3.8/site-packages/segmentation_models_pytorch/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "print(len(training_loader))\n",
    "num_epochs = config['training']['epochs']\n",
    "data_iter = iter(training_loader)\n",
    "\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times, start with one single epoch\n",
    "    print(f'epoch {epoch} / {num_epochs}')\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # TODO model.train() # dropout, batchnorm behave differently if train vs eval\n",
    "        \n",
    "    for i, batch in enumerate(training_loader):\n",
    "        # basic training loop\n",
    "        \n",
    "        # conv expectes (n_samples, channels, height, width) # e.g., (1000, 1, 224, 224)\n",
    "        # Passing grayscale images in their usual format (224, 224) won't work.\n",
    "        # Need to add one channel dimension\n",
    "        # https://stackoverflow.com/questions/57237381/runtimeerror-expected-4-dimensional-input-for-4-dimensional-weight-32-3-3-but\n",
    "        inputs, labels = batch['frame_cropped'], batch['label_cropped'] \n",
    "        optimizer.zero_grad(set_to_none=False)\n",
    "        \n",
    "        # does model have flatten layer?\n",
    "        # Flatten images into a xxx long vector\n",
    "        #inputs = inputs.view(inputs.shape[0], -1)\n",
    "        \n",
    "        print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Every 1000 mini-batches...\n",
    "            print('Batch {}'.format(i + 1))\n",
    "            # Check against the validation set\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            model.train(False) # Don't need to track gradents for validation\n",
    "            for j, vdata in enumerate(validation_loader, 0):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "            model.train(True) # Turn gradients back on for training\n",
    "\n",
    "            avg_loss = running_loss / 100\n",
    "            avg_vloss = running_vloss / len(validation_loader)\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch * len(training_loader) + i)\n",
    "\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = 2\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, output_channels, 3)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(output_channels, output_channels, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(572,572)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=12)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
