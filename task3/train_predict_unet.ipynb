{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6525f453",
   "metadata": {},
   "source": [
    "# Task 3 Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b978add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task3.utils.config import *\n",
    "from tqdm import tqdm\n",
    "from task3.utils.data_utils import evaluate, save_zipped_pickle, load_zipped_pickle\n",
    "from task3.utils.img_utils import show_img_batch, plot_pred_on_frame\n",
    "from task3.utils.utils import upscale, get_img_dims\n",
    "import pickle\n",
    "import gzip\n",
    "import importlib\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import IoU\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2fad482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = init(config='configs/train_tg.yaml')\n",
    "config = init(config='configs/raphaela.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1241af70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-03T14:43:52.219467+0100 INFO You are using cpu.\n",
      "2022-01-03T14:43:52.219981+0100 WARNING You are using cpu but system device was found to be cuda. Check your device choice in config.py.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(config['device'])\n",
    "sys_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "logger.info(f'You are using {device}.')\n",
    "\n",
    "if device != sys_device:\n",
    "    logger.warning(f'You are using {device} but system device was found to be {sys_device}. Check your device choice in config.py.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18249aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-03T14:43:52.487357+0100 INFO model params set to: {'encoder_name': 'resnet34', 'encoder_weights': None, 'in_channels': 1, 'classes': 1, 'encoder_depth': 5, 'decoder_use_batchnorm': True, 'decoder_channels': [256, 128, 64, 32, 16], 'decoder_attention_type': 'scse', 'activation': 'sigmoid', 'aux_params': None}\n"
     ]
    }
   ],
   "source": [
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431f1ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-03T14:43:52.493401+0100 DEBUG selected device: cpu\n",
      "2022-01-03T14:43:52.493903+0100 DEBUG Dataset creation: train\n",
      "2022-01-03T14:43:58.618561+0100 DEBUG dict_keys(['train', 'val', 'test', 'submission'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 201/1560 [00:00<00:00, 68939.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-03T14:43:58.655144+0100 DEBUG Dataset creation: validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-03T14:44:04.196510+0100 DEBUG dict_keys(['train', 'val', 'test', 'submission'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:00<00:00, 122502.70it/s]\n"
     ]
    }
   ],
   "source": [
    "training_loader, validation_loader, test_loader = get_data_loader(config, mode='train', get_subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53160ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-03T14:44:04.234437+0100 INFO Using BCELoss() as loss function.\n"
     ]
    }
   ],
   "source": [
    "optimizer = get_optimizer(model, config)\n",
    "criterion = get_loss(model, config)\n",
    "\n",
    "# learning rate scheduler TODO add to config\n",
    "# decays lr after 10 epochs by factor 0.1, e.g. from 0.005 to 0.0005 every 10 epochs\n",
    "\n",
    "lr_scheduler = get_lrscheduler(optimizer, config)\n",
    "#lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "#torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10,\n",
    "#                                           threshold=0.0001, threshold_mode='rel', cooldown=0,\n",
    "#                                           min_lr=0, eps=1e-08, verbose=False)\n",
    "num_epochs = config['training'].get('epochs', 1)\n",
    "save_path = config['training'].get('save_path', 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9cff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize metric\n",
    "metric = IoU(num_classes=2) # num classes in Unet=1 for binary segmentation, corresponds to 2 in IoU score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2dd65",
   "metadata": {},
   "source": [
    "## Set up tensorboard and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3b72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d62bc082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1413"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(save_path+'/config.txt', 'w+')\n",
    "f.write(config['run_notes'])\n",
    "f.write('\\n\\n')\n",
    "f.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf59fc",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38cab4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    running_score = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, batch in enumerate(tqdm(training_loader)):\n",
    "        inputs, labels = batch['frame_cropped'], batch['label_cropped']\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        labels_fl = labels.float()\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels_fl) # if BCE we need floats (0.0 or 1.0) for labels\n",
    "        # we need a threshold when calcualting IoU as we have a sigmoid output -> [0,1] but we need (0,1)\n",
    "        outputs_thr = (outputs > 0.5)\n",
    "        score = metric(outputs_thr, labels) # here we need bool for labels not float\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        running_score += score.item()\n",
    "        \n",
    "        # report average per-batch loss of last for last ... batches\n",
    "        if i % 5 == 4:\n",
    "            last_loss = running_loss / 5 # loss per batch\n",
    "            last_score = running_score / 5 # IoU per batch\n",
    "            #print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            #print('  batch {} IoU: {}'.format(i + 1, last_score))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('IoU/train', last_score, tb_x)\n",
    "            running_loss = 0.\n",
    "            running_score = 0.\n",
    "\n",
    "    return last_loss, last_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df3b98",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 400 epochs\n",
      "batch size:  8\n",
      "saving results and models to runs/mv_training_15\n",
      "training model...\n",
      "\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [01:01<00:34,  3.88s/it]"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(save_path+'/mitrial_valve_{}'.format(timestamp))\n",
    "\n",
    "\n",
    "\n",
    "print(f'training for {num_epochs} epochs')\n",
    "print(f'batch size: ', config['data'].get('batch_size', None))\n",
    "print(f'saving results and models to {save_path}')\n",
    "print('training model...')\n",
    "\n",
    "start = time.time()\n",
    "epoch_number = 0\n",
    "best_vloss = 1_000_000.\n",
    "best_vscore = 1_000_000.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('\\nEPOCH {}:'.format(epoch_number + 1))\n",
    "    start_epoch = time.time()\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, avg_score = train_one_epoch(epoch_number, writer)\n",
    "    \n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vscore = 0.0\n",
    "\n",
    "    # validate model for every epoch\n",
    "    for i, vbatch in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vbatch['frame_cropped'], vbatch['label_cropped']\n",
    "        voutputs = model(vinputs)\n",
    "        vlabels_fl = vlabels.float()\n",
    "        vloss = criterion(voutputs, vlabels_fl)\n",
    "        voutputs_thr = voutputs > 0.5\n",
    "        vscore = metric(voutputs_thr, vlabels)\n",
    "        \n",
    "        running_vloss += vloss\n",
    "        running_vscore += vscore\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vscore = running_vscore / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('IOU train {} valid {}'.format(avg_score, avg_vscore))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation IoU',\n",
    "                    { 'Training' : avg_score, 'Validation' : avg_vscore },\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    # write last figure of batch to tensorboard\n",
    "    fimg_grid = torchvision.utils.make_grid(vinputs)\n",
    "    limg_grid = torchvision.utils.make_grid(vlabels)\n",
    "    pimg_grid = torchvision.utils.make_grid(voutputs_thr.detach())\n",
    "    \n",
    "    writer.add_image(f'{save_path}_epoch_{epoch_number}_frame_valiou_{vscore}', fimg_grid)\n",
    "    writer.add_image(f'{save_path}_epoch_{epoch_number}_label_valiou_{vscore}', limg_grid)\n",
    "    writer.add_image(f'{save_path}_epoch_{epoch_number}_pred_valiou_{vscore}', pimg_grid)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state, could also use IoU instead of loss \n",
    "    # Or use Jaccard loss as it is a direct proxy to IoU\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = save_path + '/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    epoch_duration = (time.time()-start_epoch)/60\n",
    "    print(f'Epoch {epoch_number} finished in {epoch_duration} min')\n",
    "\n",
    "    epoch_number += 1\n",
    "\n",
    "duration = (time.time()-start)/60\n",
    "print(f'\\nTraining finished in {duration} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3fcbf1",
   "metadata": {},
   "source": [
    "## Restore model and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f6d10",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "load_dir = 'runs/mv_training_7/model_20211230_133722_20'\n",
    "submission_loader = get_data_loader(config, mode='submission', get_subset=False)\n",
    "#model = smp.Unet(**config['model'].get('smp-unet'))\n",
    "model = get_model(config) \n",
    "model.to(device) # if not called wrong input shape (unclear why)\n",
    "model.load_state_dict(torch.load(load_dir))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c21977",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lookup_test = pd.read_csv('data/lookup_test.csv')\n",
    "\n",
    "predictions_jodok = []\n",
    "predictions = []\n",
    "\n",
    "for i, batch in enumerate(submission_loader):\n",
    "    \n",
    "    frame = batch['frame_cropped']\n",
    "    name = batch['name']\n",
    "    \n",
    "    fid = batch['id']\n",
    "    prediction = model(frame)\n",
    "    prediction_thr = prediction > 0.5\n",
    "\n",
    "    img_dims = get_img_dims(lookup_test, name[0])\n",
    "    lookup_sample = lookup_test[lookup_test.name == name[0]]\n",
    "    roi_coord = lookup_sample.roi_coord.values[0]\n",
    "    roi_dims = lookup_sample.roi_dims.values[0]    \n",
    "\n",
    "    roi_coord_stripped = roi_coord.replace('(', '')\n",
    "    roi_coord_stripped = roi_coord_stripped.replace(')', '')\n",
    "\n",
    "    roi_dims_stripped = roi_dims.replace('(', '')\n",
    "    roi_dims_stripped = roi_dims_stripped.replace(')', '')    \n",
    "\n",
    "    roi_coord_final = tuple(map(int, roi_coord_stripped.split(', ')))\n",
    "    roi_dims_final = tuple(map(int, roi_dims_stripped.split(', ')))\n",
    "\n",
    "    #print('roi coord final', roi_coord_final)\n",
    "    #print('roi dims final', roi_dims_final)        \n",
    "    #print(fid, '\\n')\n",
    "    #print('orig_frame_dim', batch['orig_frame_dims'])\n",
    "    #print('frame shape', frame.shape)\n",
    "    #print('pred shape', prediction_thr.shape)\n",
    "    #print('img_dims', img_dims)\n",
    "    pred_squeezed = prediction_thr.squeeze(0).squeeze(0)\n",
    "    pred_scaled = upscale(pred_squeezed.numpy().astype(np.uint8), img_dims, roi_coord_final, roi_dims_final)\n",
    "\n",
    "    #print(pred_scaled.shape)\n",
    "    # name needs to be string\n",
    "    # prediction needs to be 2D numpy bool array\n",
    "    \n",
    "    ## raws for jodok\n",
    "    #predictions_jodok.append({\n",
    "    #   'name': name[0],\n",
    "    #    'prediction_upscaled': pred_scaled.astype(bool),\n",
    "    #    'prediction_raw': pred_squeezed.to(bool)\n",
    "    #    }\n",
    "    #)\n",
    "\n",
    "    predictions.append({\n",
    "       'name': name[0],\n",
    "        'prediction': pred_scaled.astype(bool),\n",
    "        }\n",
    "    )    \n",
    "\n",
    "    #print('pred scaled shape', pred_scaled.shape) # shoud be 2Dim    \n",
    "\n",
    "    #fig = plt.figure(figsize=(8,6))\n",
    "    #ax = fig.subplots(1,2)\n",
    "    # original frame\n",
    "    #ax[0].imshow(frame_squeezed, interpolation='nearest')\n",
    "    #ax[1].imshow(pred_squeezed, interpolation='nearest')\n",
    "    #ax[0].set_title(f'name_{name}_video_{i}_frame_{fid}')\n",
    "    #plt.savefig(f'plots/frame_and_predictions_name_{name}_idx_{i}_frame_{fid}.png')\n",
    "    #show_img_batch(batch, pred=prediction_thr.detach())\n",
    "    #print('\\n----------------------------------------\\n')\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2cd5e1",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "save_zipped_pickle(predictions, 'submissions/mv_run7_unet++_resnet34_attention_predictions.pkl')\n",
    "save_zipped_pickle(predictions_jodok, 'submissions/mv_run7_unet++_resnet34_attention_predictions_jodok.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a013513",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(predictions)\n",
    "\n",
    "def mySum(dataframe):\n",
    "    return np.stack(dataframe, axis=2) # make sure dims are correct (image_x, image_y, num_frames)\n",
    "\n",
    "res = df.groupby('name').agg(mySum).reset_index(drop=False)\n",
    "submissions_corrected = pd.DataFrame.to_dict(res, orient='records')\n",
    "\n",
    "\n",
    "sort_order = {'RZ9W7OK2EO': 0,\n",
    "              '401JD35E1A': 1,\n",
    "              'O7WUJ71C15': 2,\n",
    "              '7UXIXUBK2G': 3,\n",
    "              'JQX264DTZ0': 4,\n",
    "              'NHC30J31YN': 5, \n",
    "              'CD4RIAOCHG': 6,\n",
    "              'QJTAVYCG6M': 7,\n",
    "              '3WOQKZBVRN': 8,\n",
    "              'UB7LFQKZT5': 9,\n",
    "              'SZKYOVQ4ZP': 10,\n",
    "              'ESY800XYMN': 11,\n",
    "              '1QSFD8ORNM': 12,\n",
    "              '0MVRNDWR1G': 13,\n",
    "              'VODEK84RH4': 14,\n",
    "              '1EKDG3M9L1': 15,\n",
    "              'QQW12K1U3U': 16,\n",
    "              'D271IBSMUW': 17,\n",
    "              'TYM0IJW004': 18,\n",
    "              '8FKMSXTPSJ': 19,\n",
    "             }\n",
    "submission_sorted = sorted(submissions_corrected, key=lambda d: sort_order[d['name']])\n",
    "# save in correct format, all frames aggregated into 20 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238dda69",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "save_zipped_pickle(submission_sorted, 'submissions/mv_run7_unet++_resnet34_attention_predictions_sorted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae253f4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "testset = load_zipped_pickle('data/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b6a5e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#submission = load_zipped_pickle('submissions/mv_run7_unet++_resnet34_attention_sorted_padding_corrected.pkl')\n",
    "#submission_old = load_zipped_pickle('submissions/mv_run7_unet++_resnet34_attention_correctupscaling_sorted_togroup.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a1fcf",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for subm, test in zip(submission_sorted, testset):\n",
    "\n",
    "    sname = subm['name']\n",
    "    spred = subm['prediction']\n",
    "\n",
    "    tname = test['name']\n",
    "    tframe = test['video']\n",
    "\n",
    "    print(spred.shape)\n",
    "    print(tframe.shape)\n",
    "\n",
    "    assert sname == tname\n",
    "    assert spred.shape == tframe.shape\n",
    "\n",
    "    for frame in range(tframe.shape[-1]):\n",
    "        \n",
    "        fig = plt.figure(figsize=(12,8))\n",
    "        plt.imshow(tframe[:,:,frame], alpha=1)\n",
    "        plt.imshow(spred[:,:,frame], alpha=0.6)\n",
    "        #fig2 = plt.figure(figsize=(12,8))\n",
    "        #plt.imshow(tframe[:,:,frame], alpha=1)\n",
    "        #plt.imshow(spredold[:,:,frame], alpha=0.6)\n",
    "        \n",
    "        #plot_pred_on_frame(tframe[:,:,frame], pred=spred[:,:,frame])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}